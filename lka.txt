关于内核的疑问
	
第1章 简介与概述
	1.1 内核的任务
		(1) 内核代替应用程序与硬件联系
		(2) 内核作为资源管理器，分配CPU时间、内存、磁盘空间、网络连接资源
		(3) 应用程序可以通过系统调用向内核发送请求
	1.2 实现策略
		1. 微内核
			(1) 微内核只实现了最核心的功能，其他所有的操作都委托给一些独立进程，这些进程通过明确定义的通信接口与中心内核通信
			(2) 该方法有很强的动态扩展性，系统的各个部分彼此都很清楚的划分开来
			(3) 缺点是进程间通信需要额外的CPU时间，由进程来委托执行操作还会造成额外的调度成本，运行效率低
		2. 宏内核
			(1) 宏内核将核心功能与所有子系统打包在一起，内核中的每个函数都可以访问内核中所有其他部分
			(2) 宏内核的灵活性相对较差，每次更新子系统都要重新编译整个内核
			(3) 但模块的引入弥补了宏内核的缺陷，通过动态的插入和移除模块提升了宏内核的灵活性，模块通过内核全局符号表访问内核关键函数，编译时通过与内核联合编译从而使用内核结构
	1.3 内核的组成部分
		1.3.1 进程、进程切换、调度
			(1) UNIX操作系统下运行的应用程序、服务器及其他程序都称为进程，进程是操作系统最关键也是最核心的概念，一切机制都围绕进程展开
			(2) 各个进程地址空间相互独立，因此进程不会意识到彼此的存在，进程间的通信需要IPC机制来实现
			(3) Linux是多任务系统，它支持并发执行若干进程，系统中同时运行的进程不会超过CPU数量，调度器会按照较短的时间在不同进程之间切换，这样就造成了同时处理多进程的假象
				[1] 进程在被抢占前要保存现场，并在重新激活运行时原样恢复从而继续执行，营造了进程独占CPU的假象
				[2] 调度器还必须确定如何在现存进程之间共享CPU时间，确定哪个进程执行多长时间的过程称为调度
		1.3.2 UNIX进程
			(1) Linux对进程采用了一种层次结构，每个进程都依赖一个父进程，内核启动init进程作为第一个进程
			(2) UNIX操作系统有两种创建新进程的机制，分别是fork和exec：
				[1] fork：创建一个新的进程，拥有独立的task_struct，并且是当前进程的副本，与当前进程之间只有PID不同，其他依赖的内核结构按需进行复制、共享、写时复制和初始化，
					写时复制的原理是将内存复制操作延迟到父进程和子进程向某内存页面写入数据之前，在只读访问模式下父进程和子进程可以共用同一内存页
				[2] exec：将一个新进程加载到当前进程的内存中执行，旧程序的内存页被刷出，并运行新程序
			1. 线程
				(1) 线程是除进程之外的另一种程序执行形式，一个进程可能由多个线程组成，他们共享地址空间，线程可以被看作是进程的一个执行流
				(2) 在调度器眼里，线程和进程没有本质区别，都是调度实体
				(3) 线程通过clone系统调用创建，拥有独立的task_struct。fork和clone都会调用体系结构无关的do_fork函数，之所以行为不同是因为clone_flag不同，fork的clone_flag
					默认是SIGCHLD。clone则可以更自由的配置clone_flag，按照配置来决定哪些资源与父进程共享，哪些资源为线程独立创建，从而实现细粒度的资源分配
				(4) 因为同一进程的线程之间共享地址空间，因此需要同步和互斥机制来避免冲突
			2. 命名空间
				(1) 命名空间使得不同的进程看到不同的系统视图，启用命名空间之后，以前的全局资源现在拥有不同分组，每个命名空间可以包含一个特定PID集合或者提供文件系统的不同视图
				(2) 通过称为容器的命名空间建立系统的多个视图，从容器内部看来这是一个完整的Linux系统，而且与其他容器没有交互
				(3) 容器是彼此分离的，每个容器实例看起来就像是运行Linux的一台计算机，一台物理机器可以运行多个这样的容器实例
		1.3.3 地址空间和特权级别
			(1) CPU的位数决定了计算机地址空间的大小，地址空间大小与实际可用的物理内存无关，因此被称为虚拟地址空间
			(2) 每个用户进程都有自己的地址空间，分为用户空间和内核空间两部分，各个进程的用户空间是彼此分离的，而内核空间则都是同样的
			(3) 在64位计算机中，倾向于使用更小的位数管理地址空间，比如42位或47位，这样会为管理地址空间节省一些工作量，也会导致地址空间中有无法寻址的空洞
			1. 特权级别
				(1) 内核把虚拟地址分成两部分，因此能保护多个系统进程，使其彼此隔离。同样为了保护系统，CPU提供了几种特权级别，限制访问硬件的指令或限制访问虚拟地址空间某一特定部分
				(2) Linux有两种不同状态，内核态和用户态，分别对应英特尔处理器的 Ring 0 和 Ring 3。处于用户态的进程禁止访问虚拟地址的内核空间，也禁止执行访问硬件的指令
				(3) 从用户态到核心态的切换需要通过系统调用完成，每当用户进程想要执行任何能影响整个系统的操作，只能通过系统调用向内核发起请求，内核首先检查进程是否允许执行想要的操作，
					然后代表进程执行所需的操作。
				(4) 内核的核心职责是 “管理整个计算机系统的硬件资源与软件资源”，并为用户进程提供一个 “安全、有序、高效的运行环境”。如果没有系统调用，用户进程像调用函数一样执行所需的
					操作，那说明同样可以编写出修改内核空间甚至随意访问硬件的恶意程序，这样的程序会破坏内核资源管理的稳定性，随意访问共享资源还会发生冲突。系统调用执行的是一系列
					固定的、安全的逻辑，它能过滤非法请求，关键是所有核心操作都交给内核来做，没有任何未知的操作，临界情况交给内核线程去处理，这样就避免了上述的攻击
				(5) 除了系统调用之外，内核还可以由异步硬件中断激活，进入中断上下文。在中断上下文中，内核无权访问用户空间，并且内核必须比正常情况更加谨慎，比如不能进入睡眠状态。况且也
					无法睡眠，因为中断程序没有对应的调度实体，它只是执行了一段程序。中断的优先级很高，要求关键操作快速处理，耗时的次要操作交给中断下半部处理
				(6) 下半部机制：中断机制有一个矛盾，即中断需要快速响应，并且中断处理过程中会屏蔽同级以下的中断，但是耗时操作会延长中断处理时间，导致同级中断不能快速响应。为了解决这个
					问题引入下半部机制，上半部快速处理硬件相关操作，尽快恢复中断响应；下半部执行耗时操作，不影响中断响应。下半部提供了四种实现方式：
					[1] 软中断：静态分配，最多32个，在硬中断里注册软中断，硬中断返回时检查并执行软中断，或者由内核线程 ksoftirqd 集中处理软中断
					[2] Tasklet：基于软中断实现（HI_SOFTIRQ），动态创建（创建的Tasklet会被放入一个链表中），不可在不同CPU并发（执行时打标记）
					[3] 工作队列：本质上是可休眠的内核线程，上半部会负责唤醒下半部内核工作线程
					[4] 延迟工作：上半部会延迟一段时间唤醒下半部内核工作线程，该延迟时间基于 jiffies 或 ktime 实现，不会阻塞上半部处理
				(7) 除用户进程外，系统中还有内核线程在运行。内核线程只能访问内核空间，无权访问用户空间。内核线程可以睡眠，也可以像用户进程一样被调度器跟踪，部分内核线程被限制了只能在
					某个CPU上运行（这和CPU亲和性与调度迁移相关）。内核线程可用于各种用途：从内存和块设备之间的数据同步，到帮助调度器在CPU上分配进程
				(8) 内核线程是直接由内核创建并运行在内核态的特殊进程，它们没有独立的用户态地址空间，只负责执行内核级的周期性任务、异步处理、资源管理等工作，以下是一些典型的内核线程：
					1. 系统核心管理类
						[1] kthreadd：内核线程管理器，是所有其他内核线程的父进程，接受来自kthread_create()的请求，创建新的内核线程并调度其运行
						[2] swapper：每个CPU都有一个swapper线程，系统idle时的“空转”进程。当CPU没有其他可运行进程时，swapper会被调度并执行cpu_idle()函数，降低CPU功耗
					2. 内存管理类
						[1] kswap：每个内存节点都有一个kswap线程，当系统物理内存不足时，kswap会周期性唤醒，通过页框回收（如将不常用的页面换出到swap分区，释放缓存页等）维持
							系统内存平衡，避免内存耗尽导致OOM
						[2] kcompactd：每个内存节点都有一个kcompactd线程，当系统中连续内存快不足时，kcompactd会将分散的小内存块合并为连续的大内存块
						[3] khugepaged：自动将进程普通页面（如4KB）合并为大页（如 2MB 或 1GB），减少页表项数量，提升CPU缓存利用率和内存访问效率
					3. 块设备与文件系统类
						[1] bdi-default：将文件系统的“脏页”异步刷新到块设备，保证内存缓存与磁盘数据的一致性，避免突然断电导致数据丢失
						[2] jbd2/*：ext3/ext4文件系统的日志管理线程，负责日志的提交、恢复等操作，确保文件项系统在崩溃后能通过日志恢复一致性，属于“日志型文件系统”的核心组件
						[3] md/*：软件 RAID 的控制线程，处理 RAID 阵列的同步、重构、故障恢复等任务，确保 RAID 设备的可靠性
					4. 中断与异步处理类
						[1] ksoftirqd/0：每个CPU都有一个ksoftirqd线程，负责集中处理下半部软中断
						[2] kworker/*：工作队列是内核中最常见的异步任务调度机制，驱动或内核模块可通过queue_work()提交任务，kworker线程负责执行这些任务。不同的kworker线程
							可能绑定到特定CPU核心，或处理特定类型的任务
					5. 其他专用线程
						[1] watchdog/0：每个CPU都有一个watchdog线程，定期向硬件watchdog发送“喂狗”信号，若内核崩溃导致线程停止运行，watchdog会触发系统重启，用于提高系统
							可靠性（常见于嵌入式设备）
						[2] 自定义内核线程：内核补丁或模块可通过kthread_create()创建自定义内核线程，完成特定任务
					6. 内核线程是内核实现“异步处理”和“后台任务”的核心机制，它们围绕资源管理（内存、文件系统）、硬件交互（块设备、中断）、系统可靠性（watchdog、日志）等核心需求设计
			2. 虚拟和物理地址空间
				(1) 系统是多进程的，并且物理内存一般要比虚拟内存小，出于对内存管理方便、内存连续性、进程隔离性的需求，使用页表建立虚拟内存和物理内存间的映射是一个绝佳的选择
				(2) 虚拟地址关系到进程的用户空间和内核空间，而物理地址则用来寻址实际可用的内存
				(3) 虚拟地址空间和物理地址空间都被分成等长的页，物理内存页称作页帧，虚拟地址页称作页
				(4) 页映射的引入使得进程之间的内存隔离有一点点松动，即两个页可以映射到同一页帧，而是否共享页帧是内核完全可控的，并扩展为更高级的机制，比如进程间通信、内核与用户共享页
				(5) 部分页没有映射的页帧，可能因为页没有使用，或者数据尚不需要使用而没有载入到内存中，还可能是页已经换出磁盘，将在需要时再换回内存
		1.3.4 页表
			(1) 将虚拟地址映射到物理地址的数据结构叫做页表
			(2) 线性页表需要占据连续的大块内存，并且每个进程都有一个页表，如果使用线性页表则系统中所有的内存都要用来保存页表，这是不切实际的
			(3) 虚拟地址空间的大部分区域都没有使用，因而也没有关联的页帧，基于这一规律可以使用更加节省内存的页表结构：多级页表
			(4) 多级页表将虚拟地址分为5个部分，分别对应全局页目录(PGD)，中间页目录(PMD)，上层页目录(PUD)，页表(PTE)，页帧的偏移量，只要有PGD的地址和虚拟地址就可以找到对应的物理地址
			(5) 多级页表对虚拟地址空间中不需要的区域，不必创建中间页目录或页表，从而大量节省内存
			(6) CPU中的MMU单元会自动根据页表进行虚拟地址的映射，这大大节省了内核的工作。地址转换中频繁出现的地址，保存在称为地址转换后备缓冲器(TLB)的CPU高速缓存中，进程切换或者更新页表项都
				要刷新TLB，这个工作在许多体系结构中是自动的，在少部分体系结构中要内核执行，因此内核凡涉及操作页表之处都必须调用相应的指令，如果针对不需要此类操作的体系结构编译内核，则相应
				的宏调用自动变为空操作
			1. 与CPU的交互
				(1) 部分体系结构只有两层或三层页表，而Linux默认是四层页表，因此体系结构相关的代码需要将多出来的页表设置为空表从而适配体系结构无关的代码
			2. 内存映射
				(1) 内存映射是一种重要的抽象手段，在内核中大量使用，也可用于用户应用程序。映射方法可以将任意来源的数据传输到进程的虚拟地址空间中。作为映射目标的地址空间区域，可以像普通
					内存那样用通常的方法访问，但任何修改都会自动传输到原数据源
				(2) 内核在实现设备驱动程序时直接使用了内存映射，外设的输入/输出可以映射到虚拟地址空间的区域上，可以直接访问。并且由于Linux将所有硬件设备都看作是文件，因此对相关内存区域
					的修改会通过写回机制重定向到硬件设备，大大简化了驱动程序的实现
				(3) 我认为内存映射其实就是页帧共享，内核与设备共享页帧，通过写回机制同步；内核与用户进程共享页帧；进程和进程之间共享页帧；进程与文件之间共享页帧等等
		1.3.5 物理内存的分配
			(1) 内核分配内存时，必须记录页帧的已分配或空闲状态，以免两个进程分配同样的内存区域。由于内存分配和释放非常频繁，内核还必须保证相关操作尽快完成。内核可以只分配完整的页帧，标准库将
				来源于内核的页帧拆分为小的区域，并为进程分配内存
			1. 伙伴系统
				(1) 伙伴系统是分配连续空闲页帧的绝佳方法
				(2) 空闲内存块总是两两分组，每组的两个内存块称为伙伴，伙伴的分配是彼此独立的，如果两个伙伴都是空闲的，内核会将其合并为一个更大的内存块
				(3) 内核对所有大小相同的内存块放在同一列表中管理，每一层的内存块大小都是2的指数，在已知内存块大小和其中一个伙伴的地址的情况下，可以通过位运算快速得出另一个伙伴的地址
				(4) 分配内存时，大的内存块会分裂直至获得所需的内存块，其余的内存块放回伙伴系统
				(5) 释放内存时，内核可以检查地址，来判断是否能够创建一组伙伴，并合并为一个更大的内存块放回到伙伴系统中
				(6) 系统长期运行会导致内存碎片增加，即系统中有若干页帧是空闲的，但是却分散在物理地址空间的各处。比如一个大内存块中间的一个页帧被使用，那么两侧的空闲内存块是没办法合并的，
					系统运行的时间越长这种情况发生的就越多，伙伴系统可以减少这种效应，并且内核还增加了一些有效措施来防止内存碎片
			2. slab缓存
				(1) 内核本身经常需要比完整页帧小得多的内存块。由于内核无法使用标准库中的函数，因而必须在伙伴系统基础上自性定义额外的内存管理层，将伙伴系统提供的页划分为更小的部分，即slab
					缓存，该方法不仅可以分配内存，还为频繁使用的小对象实现了一个一般性的缓存
				(2) 对频繁使用的对象，内核定义了只包含所需类型对象实例的缓存。每次需要每种对象时，可以从对应的缓存中快速分配，还可以进行对象预初始化等优化工作
				(3) 对通常情况下小内存块的分配，内核针对不同大小的对象定义了一组slab缓存，分配内存时会根据对象的大小选择合适的slab缓存
				(4) slab分配器在各种工作负荷下的性能都很好，但在真正规模庞大的超级计算机上使用时，出现了一些可伸缩性问题；在真正微小的嵌入式设备上(物理内存很小)，slab分配器的开销又太大，
					内核提供了两种slab分配器的备用方案从而适配以上两种场景。以上三种方案的接口相同，因此不用关注内核实际编译进来的底层分配器是哪一个
			3. 页面交换和页面回收
				(1) 页面交换通过利用磁盘空间作为扩展内存，从而增加了可用的内存
				(2) 在内核需要更多内存时，不常用的页可以写入硬盘。如果再需要访问相关数据，内核会将相应的页切换回内存
				(3) 通过缺页异常机制，这种切换操作对应用程序是透明的。换出的页可以通过特殊的页表项标识，在进程试图访问此类页帧时，CPU会启动一个可被内核截取的缺页异常，此时内核可以将硬盘上
					的数据切换到内存中，接下来用户进程恢复运行。用户进程无法感知到缺页异常，因此页面交换对进程来说是透明的
				(4) 页面回收用于将内存映射被修改的内容与底层的块设备同步，有时也叫数据回写。数据刷出后，内核即可将页帧用于其他用途
		1.3.6 计时
			(1) 内核必须能够测量时间以及不同时间点的时差，比如进程调度就会用到该功能，可以使用jiffies和高精度定时器
			(2) 内核有两个全局变量，jiffies_64和jiffies，jiffies是jiffies_64的低32位，对jiffies_64加一就是等效地对jiffies加一，这是一个天才般的设计
			(3) 定时器中断会根据HZ设定的频率触发，每次定时器中断都会递增jiffies_64，处理到期的软件定时器，触发周期调度器，更新系统时间（墙上时间）
			(4) 定时器内置在CPU中，有其固定的频率，定时器内部的计数器设置为 (定时器频率 / HZ)，每周期递减计数器，计数器归零时触发定时器中断，中断处理结束后重置计数器
			(5) 高精度定时器不是周期定时器，而是将所有已注册的到期时间放入到一个红黑树中，每次有新定时器注册时，都会更新红黑树
			(6) 内核根据最早到期的定时器时间，通过硬件接口设置下一次中断的触发时间，当硬件定时器触发中断时，内核会遍历红黑树，执行所有到期定时器的回调函数，并更新红黑树
			(7) 虽然高精度定时器不是周期定时器，但是可以在回调函数中设置下一周期的到期时间
		1.3.7 系统调用
			(1) 系统调用是内核与用户交互的经典方法，传统的系统调用按照不同类别分组：
				[1] 进程管理：创建新进程，查询信息，调试
				[2] 信号：发送信号，定时器以及相关处理机制
				[3] 文件：创建、打开和关闭文件，从文件读取或向文件写入
				[4] 目录和文件系统：创建、删除和重命名目录，查询信息，链接，变更目录
				[5] 保护机制：读取和变更UID/GID，命名空间的处理
				[6] 定时器：定时器函数和统计信息
			(2) 系统调用不能以标准库形式实现，因为需要特别的保护机制保证系统稳定性和安全不受危及。此外许多调用依赖内核内部的结构或函数，这也导致了无法在用户空间实现
			(3) 发出系统调用时，处理器必须改变特权级别，从用户态进入内核态。这一操作有很多步骤，比如sp指向内核栈、用户态寄存器压栈、更新特权级别等等，体系结构一般会提供专用的汇编指令实现从
				用户态到内核态的转变（或者直接利用中断指令，int 0x80h）
		1.3.8 设备驱动程序、块设备和字符设备
			(1) 设备驱动程序用于与系统连接的输入/输出设备通信。硬件连接到计算机上，就会与计算机发生交互，也就是输入/输出
			(2) 在Linux眼中，万物皆文件，包括外部设备。应用程序可以通过访问/dev下的文件和设备进行交互。设备驱动程序的任务就是支持应用程序经由设备文件与设备通信
			(3) 外部设备分成以下两类：
				[1] 字符设备：提供连续的数据流，应用程序可以顺序读取，通常不支持随机存取
				[2] 块设备：应用程序可以随机访问设备数据，程序可自行取定读取数据的位置。硬盘是典型的块设备，应用程序可以寻址磁盘的任何位置，但是只能以块为单位
			(4) 应用程序与外部设备交互时，只会对设备文件使用read/write系统调用，设备驱动程序负责接受用户请求，并与外设交互
			(5) 当read调用时无数据可读，进程会进入“可中断睡眠”状态，并将进程加入设备驱动的等待队列，设备驱动程序接受到数据后，会唤醒等待队列中的进程
			(7) 编写块设备的驱动程序比字符设备困难得多，因为内核为提高系统性能广泛使用了缓存机制
		1.3.9 网络
			(1) 网卡也通过设备驱动程序控制，但在内核中属于特殊情况，因为不能直接通过网卡设备文件访问网络，因为应用程序发送或接受的数据要经过协议层打包或拆包
			(2) 为支持用户进程通过文件接口处理网络链接，Linux引入了套接字抽象。用户进程通过套接字文件访问网络，在套接字处理程序中打包数据，然后通过网卡设备驱动程序发送数据，反之亦然
		1.3.10 文件系统
			(1) Linux系统由成千上万的文件组成，这些文件被存储在块设备中。存储使用了层次式文件系统
			(2) Linux支持许多不同的文件系统，包括Ext2, Ext3, ReiserFS, XFS, VFAT，不同文件系统的概念抽象可以说是南辕北辙。文件系统归根结底还是软件逻辑，依赖块设备驱动程序
			(3) Ext2基于inode，inode中存储了文件所有的元信息，以及指向相关数据块的指针。目录可以表示为普通文件，其数据包括了指向目录下所有文件的inode的指针，从而建立层次结构
			(4) 内核还必须提供一个额外的软件层，将各种底层文件系统的具体特性与应用层（和内核自身）隔离起来，该软件层称为VFS（虚拟文件系统）
			(5) VFS既是向下的接口（所有文件系统都必须实现该接口），同时也是向上的接口（系统调用程序通过VFS接口访问文件系统功能）
		1.3.11 模块和热拔插
			(1) 模块用于在运行时动态向内核添加功能，包括设备驱动、文件系统、网络协议等，实际上内核任何除基本功能的子系统都可以模块化。这消除了宏内核与微内核相比一个重要的不利之处
			(2) 模块本质上不过是普通的程序，只不过运行在内核空间，模块必须提供某些代码段 在模块初始化（和终止）时执行，以便向内核注册和注销模块，模块和普通内核代码一样拥有最高权限，理论上
				能够访问所有内核函数和数据，但是标准的做法是模块只能访问内核暴露在全局符号表中的函数和数据，否则会编译失败或者无法被社区承认，并且模块编译需要用到内核的头文件
			(3) 对于支持热拔查而言，模块在本质上是必需的。系统检测到新设备时，通过加载对应的模块，可以将必要的驱动程序自动添加到内核中。模块特性使得内核可以支持种类繁多的设备，而内核
				自身在内存的大小不会发生膨胀
			(4) 因为模块时运行在内核态的，拥有最高权限，理论上可以任意修改内核数据，因此恶意模块或者未经社区审查的模块可能会造成系统崩溃
		1.3.12 缓存
			(1) 内存作为块设备的缓存，缓存按页组织，也叫页缓存
		1.3.13 链表处理
			(1) 出于对扩展性和最大化利用内存空间的需求，内核普遍使用链表结构
			(2) 内核对链表的设计十分巧妙，通过将链表结构嵌入到数据结构中，可以将任何类型的数据用链表连接起来，用container_of机制获取链表元素的位置
			(3) 链表都有一个表头，并且是双向循环链表，可以在O(1)时间内访问第一个或最后一个元素
			(4) 内核还提供了若干函数操作链表，包括插入、删除元素，检查空链表，合并链表，查找链表元素，遍历链表
		1.3.14 对象管理和引用计数
			(1) 内核中许多地方需要跟踪C语言中的结构，为此内核使用一般性的方法管理内核对象，从而避免代码复制，同时也为内核不同部分管理的对象提供了一致的视图
			(2) 一般性的内核对象机制需要以下操作：
				[1] 引用计数
				[2] 管理对象链表（集合）
				[3] 集合加锁
				[4] 将对象属性导入到用户空间（通过sysfs文件系统）
			1. 一般性的内核对象
				(1) kobject结构嵌入到其他数据结构中，作为内核对象的基础
				(2) kobject结构的成员含义如下所示：
					[1] k_name：对象的名称，可利用sysfs导出到用户空间，sysfs对应的就是根目录的sys文件夹
					[2] kref：用以简化引用计数的管理
					[3] entry：链表节点，用于将若干内核对象放置到一个链表中
					[4] kset：将对象与其他对象放置在一个集合时，需要用到kset
					[5] parent：指向父对象的指针，用于在sysfs中建立层次结构
					[6] ktype：提供了包含kobject的数据结构的更多详细信息
				(3) 内核提供了若干函数管理kobject，实际上是作用于包含kobject的结构：
					[1] kobject_get, kobject_put：对kobject的引用计数器加1或减1
					[2] kobject_(un)register：从层次结构中注册或删除对象，对象被添加到父对象中现存的集合中（如果有的话），同时在sysfs文件系统中创建一个对应项 
					[3] kobject_init：初始化一个kobject，即将引用计数器设置为初始值，初始化对象的链表元素
					[4] kobject_add：初始化一个内核对象，并使之显示在sysfs中
					[5] kobject_cleanup：在不需要kobject（以及包含kobject的对象）时，释放分配的资源
				(4) kref结构将一个原子类型变量封装在结构中，防止直接操纵值。必须使用kref_init初始化kref；如果要使用某个对象，必须调用kref_get对引用计数器加1；如果对象不再使用，
					则必须调用kref_put使引用计数器减1
			2. 对象集合
				(1) 内核对象集合机制使用kset结构，其成员含义如下：
					[1] ktype：指向kset中各个内核对象公用的kobj_type结构
					[2] list：属于当前集合的内核对象的链表表头
					[3] uevent_ops：提供了若干函数指针，用于将集合的状态信息传递给应用层
					[4] kobj：管理集合的结构只能是内核对象，因此嵌入kobject管理kset对象本身。也就是说集合下面仍然可以有集合，组成了一个类似文件系统的层次结构
			3. 引用计数
				(1) 引用计数用于检测内核中有多少地方使用了某个对象，每次新的引用都要对计数器加1，不再需要引用则计数器减1，引用计数为0时释放对象
				(2) 引用计数的数据结构很简单，只提供了一个原子引用计数。这里的“原子”意味着对该变量的加1和减1操作在多处理器系统上也是安全的
				(3) 使用kref_init、kref_get和kref_put用于对引用计数器进行初始化、加1、减1操作
		1.3.15 数据类型
			1. 类型定义
				(1) 内核使用typedef来定义各种数据结构，以避免依赖于体系结构相关的特性。为此内核定义了若干整数数据类型，不仅明确了是否有符号，还指定了相关类型的精确位数
			2. 字节序
				(1) 现代计算机采用大端序或小端序，内核提供了各种函数和宏，可以在CPU使用的格式与特定的表示法之间转换
			3. per-cpu变量
				(1) 通过DEFINE_PER_CPU(name, type)为每个CPU创建一个type类型的实例，通过get_cpu(name, cpu)获取对应的实例，smp_processor_id()返回当前活动处理器的ID
				(2) 适用于需要为每个CPU创建实例的场景，比如就绪队列
			4. 访问用户空间
				(1) 内核代码权限很高，理论上可以访问地址空间上任意位置，但是为了避免内核代码破坏用户空间，使用__uesr标识防止内核代码随意访问用户空间
				(2) 来自用户空间的地址必须加上__user标识，内核不能直接访问带有该标识的地址，必须通过特定的函数访问，否则会编译错误
				(3) 地址对应的用户空间页帧可能不在物理内存中，这也是内核不能直接访问用户空间的原因，使用特殊的函数可以在访问之前进行详细的检查
		1.3.16 本书的局限性
			(1) 内核开发是一个高度动态的过程，内核获得新特性和持续改进的速度有时简直是不可思议
	1.4 为什么内核是特别的
		(1) 内核很神奇，但归根结底它只是一个巨大的C程序，带有一些汇编代码（不时出现很少量的“黑巫术”）
		(2) 内核是由世界上最好的程序员编写的，源代码可以证实这一点，其结构良好，细节一丝不苟，巧妙的解决方法在代码中处处可见。一言以蔽之：内核应该是什么样子，它现在就是什么样子
	1.5 行文注记
		(1) 待在孤岛上并不有趣
	1.6 小结
		(1) 在人类编写过的所有软件中，Linux内核无疑是最有趣和最吸引人的一种软件了
第2章 进程管理和调度
	(1) 所有操作系统都能够同时运行多个进程，至少用户错觉上是这样。真正并行运行的进程数量取决于物理CPU数量
	(2) 内核和处理器建立了多任务的错觉，通过在短时间内切换进程，在感官上觉得计算机并行执行多个进程。这引出了如下两个问题：
		[1] 进程之间不能彼此干扰
		[2] CPU时间必须在各个进程之间尽可能公平的共享，其中一些进程比其他进程更重要
	(3) 对于第一个问题可以通过页表+虚拟内存实现，第二个问题则由调度器处理，调度器有如下两个任务：
		[1] 调度策略：调度器要决定为进程分配多长时间，何时切换到下一进程。这又引出了哪个进程是下一个进程的问题。此类决策时体系结构无关的
		[2] 任务切换：在内核从进程A切换到进程B时，必须保证进程B的执行环境与上一次撤销其处理器时完全相同。执行环境是指寄存器的内容和虚拟地址空间的结构，这个工作与体系结构相关
	2.1 进程优先级
		(1) 进程可以根据关键度分为三类：
			[1] 硬实时进程：有严格的时间限制，某些任务必须在规定的时限内完成。Linux本身不支持硬实时进程，修改过后的内核通过类似微内核的方式实现了硬实时，将次要的内核工作交给独立进程处理，
				硬实时进程拥有最高的优先级，将抢占其他所有类型进程并执行
			[2] 软实时进程：是硬实时进程的一种弱化形式。软实时进程优先级优于普通进程，任务超时会写入内核日志，不会导致内核崩溃或者世界末日
			[3] 普通进程：没有特殊约束，使用完全公平调度策略，根据重要性分配优先级
			[4] 在设计调度策略时，要考虑到如下几个问题：
				(1) 重要的进程要比次要的进程分配到更多的CPU时间
				(2) 因为等待资源而睡眠的进程不应该被调度
				(3) 必须支持不同的调度类别（如实时调度和完全公平调度）
				(4) 在某些情况下，更重要的进程会抢占次重要的进程（如实时进程抢占普通进程）
			[5] 完全公平调度器(CFS)：尽可能模仿理想情况下的公平调度，并且调度更加一般性的调度实体。比如在调度器分配时间时，可以首先在不同用户之间分配，接下来在各个用户的进程之间分配
	2.2 进程生命周期
		(1) 进程并不总是可以立即运行，有时它必须等待来自外部信号源，不受其控制的事件。这里的等待不是主动等待，而是被动唤醒
		(2) 进程有以下几种状态：
			[1] 运行：此刻进程正在运行
			[2] 等待：进程能够运行，但是没有分配CPU。调度器可以在下一次任务切换时选择该进程
			[3] 睡眠：进程正在睡眠无法运行，它在等待外部事件唤醒
		(3) 系统将所有进程保存在一个进程表中，无论其状态是运行、睡眠还是等待。睡眠进程会分类到对应的队列中，发生事件时将进程唤醒，这就是事件驱动的原理
		(4) 进程还有一个特殊状态就是“僵尸”状态，即进程的资源（内存、与外设的连接，等等）已经释放，它们无法也决不能再次运行，但是进程表中仍然有对应的表项
		(5) 进程的父进程在子进程终止时必须调用或已经调用wait4（wait for）系统调用，因为要想内核证实父进程已经确认子进程的终结。为什么要采用这么奇怪的方式呢？因为由于进程的隔离性，父进程看不到
			子进程的状态，它只知道子进程的PID，因此子进程终结后会向父进程发送信号，父进程回收资源并申请wait4系统调用彻底删除子进程。也就是说，进程终结后，由内核来回收内核层面的资源，由父进程
			回收用户层面的资源，然后才能在进程表中彻底删除进程并回收task_struct结构。假如进程终结后直接回收全部资源，那么由于父进程只知道子进程的PID，而该PID被新的进程占用，导致父进程的错误，
			这也是使用wait4系统调用的原因之一
		(6) 如果父进程不使用wait4系统调用，那么僵尸进程将稳定地存在于进程表中，但由于僵尸进程残余的数据在内核中占据的空间极少，只在需要长时间运行的网络服务器上才会有问题
		(7) 如果父进程终结，其子进程（包括僵尸进程）会将init进程作为自己的父进程，init进程会发起wait4系统调用回收终结的子进程
		抢占式多任务处理：
			(1) Linux中执行的代码三种执行上下文，初始化上下文、进程上下文、中断上下文，进程上下文又有两种状态，即用户态和内核态，初始化全程在内核态中执行，它们有不同的抢占级别：
				[1] 用户态：用户态进程总是可能被抢占，其可能在任意位置被中断抢占，并且在内核态返回用户态时被其他进程抢占
				[2] 内核态：如果系统处于内核态，那么系统中的其他进程是无法将其抢占的，调度器必须等待内核代码执行结束返回用户态时，才能切换到另一个进程执行，中断可以抢占内核态进程
				[3] 中断上下文：中断可以暂停处于用户态或内核态的进程，中断拥有最高优先级，因为在中断触发后要尽快处理
			(2) 上面不同的抢占级别是有其原理的。CPU接收到中断信号会自动保存现场并跳转到对应的代码，因此中断的抢占优先级最高；之所以在内核态返回用户态时才能切换到下一个进程，是因为在那个时刻
				会调用主调度器函数，该函数会检查该进程是否应该被抢占，如果是则选择下一个进程并进行上下文切换，因此在内核态代码执行时不能抢占；而用户态进程随时可能因为系统调用、定时器中断等
				原因进入内核态，并在内核态返回用户态时切换到另一个进程。
			(3) 在内核态代码进入关键的临界区时，可能会禁用中断。并且系统在进入中断上下文时会禁用同等级及以下的中断，硬件中断要快速处理上半部关键操作，下半部耗时操作交给软中断、工作线程等机制
				延时处理，从而避免阻塞后面的硬件中断
			(4) Linux引入了更加平滑的抢占机制，即内核抢占。内核态代码每次退出临界区都会先检查抢占计数，从而判断是否真正退出临界区，然后检查当前进程是否应该被调度，如果条件符合则进入主调度函数
				并进行上下文切换。进程调度都是在主调度函数中进行的，进程恢复也是继续执行主调度函数的后续代码，可以说所有被调度的进程的IP都指向同一位置
	2.3 进程表示
		(1) Linux内核涉及进程和程序的所有算法都围绕一个名为task_struct的数据结构建立，该结构定义在include/sched.h中。task_struct中有很多成员，将进程各个子系统联系起来
		(2) 可以将task_struct分为以下几个部分：
			[1] 状态和执行信息，如待决信号、使用的二进制格式、进程ID号、到父进程及其他有关进程的指针、优先级和程序执行有关的时间信息
			[2] 有关已经分配的虚拟内存的信息
			[3] 进程身份凭据，如用户ID、组ID以及权限等。可使用系统调用查询（或修改）这些数据
			[4] 使用的文件包含程序代码的二进制文件，以及进程处理的文件的所有文件系统信息
			[5] 线程信息记录该进程特定于CPU的运行时间数据
			[6] 在与其他应用程序协作时所需的进程间通信有关的信息
			[7] 该进程所用的信号处理程序，用于响应到来的信号
		(3) task_struct的state变量指定了进程的当前状态，可使用下列值：
			[1] TASK_RUNNING：进程处于可运行状态。但并不意味着分配了CPU，进程可能会一直等到调度器选中它。该状态确保进程可以立即运行，而无需等待外部事件
			[2] TASK_INTERRUPTIBLE：针对等待某事件或其他资源的睡眠进程。它们被放在对应事件的队列中，事件发生则唤醒这些进程，将状态设置为TASK_RUNNING，并将进程放入调度器的就绪队列
			[3] TASK_UNINTERRUPTIBLE：因内核指示而停用的睡眠进程。它们不能由外部信号唤醒，只能由内核亲自唤醒
			[4] TASK_STOPPED：进程特意停止运行
			[5] TASK_TRACED：因调试停止运行。用于从停止的进程中，将当前被调试的那些与常规的进程区分开来
			[6] EXIT_ZOMBIE：僵尸状态
			[7] EXIT_DEAD：指wait4系统调用已经发出，而进程完全从系统移除之前的状态。只有多个进程对同一个进程发起wait系统调用时，该状态才有意义
		(3) Linux提供资源限制机制，对进程使用系统资源施加限制，该机制利用了struct rlimit类型的rlim数组，该类型有两个成员，rlim_cur是进程当前的资源限制，rlim_max是限制的最大容许值，数组中
			每一项都对应一个资源类型：
			[1] 按毫秒计算的最大CPU时间
			[2] 允许的最大文件长度
			[3] 数据段的最大长度
			[4] (用户状态) 栈的最大长度
			[5] 内存转储文件的最大长度
			[6] 进程使用页帧的最大数目
			[7] 与进程真正UID关联的用户可以拥有的进程的最大数目
			[8] 打开文件的最大数目
			[9] 不可换出页的最大数目
			[10] 进程占用的虚拟内存的最大尺寸
			[11] 文件锁的最大数目
			[12] 待决信号的最大数目
			[13] 信息队列的最大数目
			[14] 非实时进程的优先级
			[15] 最大的实时优先级
		(4) 每用户的最大进程数，定义为max_threads/2。max_threads是一个全局变量，指定了在把八分之一可用内存用于管理线程信息的情况下，可以创建的进程数目。内核提前给定了20个线程的最小可能内存用量
                2.3.1 进程类型
                	(1) 典型的UNIX进程包括：由二进制代码组成的应用程序，单线程，分配给应用程序的一组资源。新进程是使用fork和exec系统调用产生的
                		[1] fork生成当前进程的一个副本，该副本称为子进程。原进程的资源都以适当的方式复制到子进程，因此该系统调用之后，原来的进程就有了两个独立的实例。这两个实例的联系包括：同一组
                					打开文件、同样的工作目录、内存中同样的数据等等。此外，二者别无关联
                		[2] exec从一个可执行的二进制文件中加载另一个应用程序，来代替当前运行的进程。exec并不创建新进程，因此必须使用fork复制一个旧的程序，然后用exec加载一个新的程序
                		[3] Linux还提供了新的clone系统调用，clone用于实现线程。clone的原理与fork基本相同，但新进程不是独立与父进程的，而可以与其共享某些资源。clone可以指定需要共享和复制的资源	
                					种类，相较于fork更加自由，理论上实现线程只是clone的使用场景之一，clone肯定还可以实现更多、更高级的功能
                2.3.2 命名空间
                	(1) 命名空间提供了虚拟化的一种轻量级形式，使得我们可以从不同的方面查看系统的全局属性
                        1. 概念
                   		(1) 命名空间的背景
                   			[1] 传统上，在Linux中许多资源是全局管理的，比如全局PID列表、系统属性、用户ID等。
                   			[2] 全局ID使得内核可以有选择的允许或拒绝某些特权，比如UID为0的用户拥有无限特权，其他UID的用户则不能修改其他用户的进程。
                   			[3] 虽然无法进行修改，但用户仍然可以看到其他用户的进程。
                   			[4] 这没什么问题，但在有些情况下，这种效果是不想要的，比如Web主机向用户提供Linux的全部访问权限时，向每个用户提供一个计算机成本太高，使用虚拟环境则资源分配做得
                   					不是非常好，每个用户都需要一个独立的内核，和一份完全安装好的配套的应用层应用。
					[5] 命名空间提供了一种不同的解决方案，所需资源较少。其只使用一个内核，并将所有全局资源都通过命名空间抽象起来。本质上命名空间建立了系统的不同视图
				(2) PID命名空间举例
					[1] 考虑三个PID命名空间的情况，命名空间组织为层次结构，一个命名空间是父命名空间，衍生了两个子命名空间
					[2] 每个命名空间都有自己PID为0的init进程，其他进程的PID以递增次序分配。PID在命名空间中是唯一的，但不是全局唯一的
					[3] 虽然子容器不了解系统中的其他容器，但父容器知道子命名空间的存在，也可以看到其中执行的所有进程。子命名空间的PID映射到父命名空间的PID，一个进程可能有多个PID
					[4] 如果命名空间包含的是比较简单的量，也可以是非层次的
				(3) 创建新的命名空间
					[1] 引入命名空间之前，Linux支持简单形式的命名空间，使用chroot系统调用能将进程限制到文件系统的某一部分，真正的命名空间的能控制的功能远远超过文件视图
					[2] 使用fork或clone系统调用创建新进程时，有特定的选项可以控制是与父进程共享命名空间，还是建立新的命名空间
					[3] unshare系统调用可以将当前进程的某些部分从父进程分离，其中包括命名空间
					[4] 命名空间彼此隔离，从进程的角度来看，改变全局属性不会传播到父命名空间，父命名空间的修改也不会传播到子进程。但对文件系统来说，情况则更为复杂，其中的共享机制
						非常强大，带来了大量的可能性
			2. 实现
				(1) 命名空间数据结构
					[1] 子系统的全局属性封装到命名空间中，每个进程关联到一个命名空间。每个可以感知到命名空间的子系统都必须提供一个数据结构，将所有通过命名空间形式提供的对象集中起来
					[2] struct nsproxy用于汇集指向特定子系统的命名空间包装器的指针：		
						1. UTS命名空间包含了运行内核的名称、版本、底层体系结构类型等信息
						2. 保存在struct ipc_namespace中所有与进程间通信IPC有关的信息
						3. 已经装载的文件系统视图，在struct mnt_namespace中给出
						4. 有关进程ID的信息，由struct pid_namespace提供
						5. struct user_namespace保存的用于限制每个用户资源使用的信息
						6. struct net_ns包含所有网络相关的命名空间参数
				(2) 将进程关联到命名空间	
					[1] 每个进程都内嵌一个struct nsproxy *nsproxy，从而关联到自身的命名空间视图。因为使用了指针，多个进程可以共享一组子命名空间
					[2] 如果内核编译时没有指定对具体命名空间的支持，则使用默认命名空间，其作用类似于不启用命名空间，所有的属性相当于全局的
					[3] init_nsproxy定义了初始的全局命名空间，其中维护了指向各子系统初始的命名空间对象的指针
				UTS命名空间
					(1) UTS命名空间数据结构
						[1] UTS命名空间只需要简单变量，没有层次组织
						[2] uts_namespace内含多个字符串，分别存储了系统的名称、内核发布版本、机器名等，初始设置保存在init_uts_ns中
					(2) 创建一个新的UTS命名空间 
						[1] 使用copy_utsname函数创建新的UTS命名空间，原理是生成当前uts_namespace实例的一份副本，当前进程nsproxy实例内部的指针指向这个实例
				用户命名空间
					(1) 用户命名空间数据结构
						[1] 用户命名空间数据结构内含引用计数器、哈希表和root用户的user_struct指针
						[2] 命名空间的每个用户，都有一个user_struct的实例负责记录其资源消耗，各个实例可通过uidhash_table获取
						[3] task_struct结构的user成员指向所属用户命名空间的、所属用户UID对应的user_struct实例
					(2) 创建一个新的用户命名空间
						[1] 每个用户命名空间对其用户资源统使用的统计，与其它命名空间无关，对root用户的统计也是如此。这是因为在克隆一个用户命名空间时，为当前用户和
							root都创建了新的user_struct实例
						[2] alloc_uid是clone_user_ns函数中使用的一个辅助函数，对当前命名空间中给定UID的一个用户，如果该用户没有对应的user_struct实例，则分配一个新的实例
						[3] 在为root和当前用户分别设置了user_struct实例后，switch_uid确保从现在开始将新的user_struct实例用于资源统计，实质上就是将task_struct的user
							成员指向新的user_struct实例。用户命名空间是非层次结构，子进程命名空间的所有值都是新初始化的，与父进程命名空间的值没有任何关系
		2.3.3 进程ID号
			1. 进程ID
				(1) 进程ID分类
					[1] PID：用于在其命名空间中唯一地标识进程
					[2] TGID：线程组的所有线程的统一线程组ID，与线程组长的PID相同。通过clone创建的所有线程的task_struct的group_leader成员，会指向组长的task_struct实例
					[3] PGID：独立进程可以合并成进程组，进程组成员的task_struct的pgrp属性值是相同的，即进程组长的PID。进程组简化了向组内所有成员发送信号的操作，用管道连接
						的进程包含在同一进程组中
					[4] SID：几个进程可以合并成一个会话，会话中的所有进程都有同样的会话ID，保存在task_struct的session成员中，会话可用于终端程序设计
				(2) 局部ID与全局ID
					[1] 命名空间增加了PID管理的复杂性，PID命名空间按层次组织。在建立一个新的命名空间时，该命名空间中的所有PID对父命名空间都是可见的，但子命名空间无法看到父命名空间
						的PID。这意味着某些进程可能有多个PID，凡可以看到该进程的命名空间，都会为其分配一个PID。因此，我们必须区分全局ID和局部ID
					[2] 全局ID：在内核本身和初始命名空间中的唯一ID号。对每个ID类型，都有一个给定的全局ID，保证在整个系统中是唯一的
					[3] 局部ID：属于某个特定的命名空间，只在所属的命名空间有效，不具备全局有效性
				(3) 全局ID的存储位置
					[1] 全局PID和TGID直接保存在task_struct中，分别是task_struct的pid和tgid成员
					[2] 全局PGID和SID不是直接包含在在task_struct中，而是用于信号处理的结构中。分别是task_struct->signal->__pgrp和task_struct->signal->__session
			2. 管理PID
				数据结构
					(1) PID命名空间基本结构
						[1] PID命名空间是一个层次结构，通过parent指针指向父命名空间，level描述命名空间深度，child_reaper指向命名空间的init进程。其中父命名空间可以看到
							子命名空间进程的PID，也就是说一个进程可能有多个PID。命名空间也可以理解成一个树结构，下面用树结点来代PID命名空间，从而更加抽象的理解这一概念：
							从根结点到进程所属结点的路径上的所有结点，都可以看到该进程，并且路径上每个结点都会为进程分配一个在其命名空间中唯一的ID值
						[2] 一个进程可能有多个PID，虽然它们ID值不同，但是它们所指的都是同一个进程，只是命名空间不同。因此内核将一个进程的所有ID值汇总在pid结构中，level表示
							该PID的命名空间最大深度，upid类型的numbers数组则保存不同命名空间的局部ID值，numbers数组的下标代表命名空间深度。numbers数组形式上只有一个
							数组项，但其位于pid结构末尾，因此只要分配更多空间，即可项数组添加附加项，通过level可以计算出结构的真实大小，这样做既灵活也节省了空间
						[3] 在upid结构中，ns指向对应的命名空间，nr保存了局部ID数值。表示进程在指定命名空间的ID数值
					(2) PID命名空间扩展结构
						[1] 上面已经搭建好PID命名空间的基本框架，但是由于具体需求，还需要对该结构进行扩展
						[2] 对于task_struct结构来说，ID有三种类型，即PID、PGID、SID，因此至少需要内嵌三个pid指针。而且必须在已知pid实例的情况下，能够获取到使用该pid的
							task_struct实例，但是由上可知，可能会有多个task_struct共享同一个pid实例。为了实现以上需求，内核在pid结构中嵌入一个散列表，用于保存所有使用
							该pid实例的task_struct实例，散列表的每个数组项都代表一个ID类型，这样就可以在已知ID类型和pid实例的情况下，找到使用该pid实例的task_struct
							实例了；内核在task_struct中内嵌了pid_link数组，pid_link有两个成员，pid指向对应的pid实例，node用作pid散列表元素，每个数组项都代表一个ID
							类型，将task_struct和pid连接起来，并且可以在已知ID类型和task_struct实例的情况下，找到对应的pid实例，然后还可以通过task_struct->ns_proxy
							->pid_namespace.level找到当前命名空间的局部ID值
						[3] 内核还有一个需求，即给出局部数字ID和命名空间，返回task_struct实例。内核中有局部ID(upid)到命名空间的联系，但是命名空间却没有保存局部ID。内核将
							所有的upid实例保存在一个名为pid_hash的哈希散列表中，散列表的大小在16~4096之间，pidhash_init用于计算恰当的容量并分配所需的内存。并且upid
							结构中也要加入一个散列表结点。内核根据(命名空间地址, 局部ID)这个全局唯一的二元组计算哈希键值，然后对比命名空间地址和局部ID找到准确的upid实例。
							通过upid实例和命名空间深度，可以找到pid实例，进程和PID是一一对应的关系，因此pid散列表中TYPE_PID散列表的第一项就是对应进程的散列表结点，从而
							可以找到对应的task_struct实例。
				函数
					(1) 核心需求
						[1] 给出局部数字ID和命名空间，查找task_struct实例
						[2] 给出task_struct、ID类型、命名空间，取得命名空间的局部ID
					(2) 给出task_struct、ID类型、命名空间，取得命名空间的局部ID
						[1] 获得与task_struct关联的pid实例，辅助函数task_pid、task_tgid、task_pgrp、task_session分别用于取得不同类型的pid实例
						[2] 在获得pid实例之后，从struct pid的numbers数组中的upid信息，即可获得局部数字ID。即pid->numbers[ns->level].nr
						[3] pid_ns_nr用于获取pid的指定命名空间的局部ID，pid_vnr用于获取pid的最深层命名空间的局部ID，pid_nr用于获取pid的全局PID
					(3) 给出局部数字ID和命名空间，查找task_struct实例
						[1] 首先使用find_pid_ns函数，根据局部ID和命名空间指针计算在pid_hash中的索引，然后遍历散列表直至找到所要的元素，通过container_of机制获取对应的upid
							实例，然后在获取到pid实例
						[2] pid_task函数取出pid->tasks[type]散列表的第一个task_struct实例
						[3] find_task_by_pid_type_ns函数在给出局部数字ID和命名空间的情况下，返回对应的task_struct实例。基于此函数，内核还创建了若干辅助函数：
							1. find_task_by_pid_ns：默认ID类型为PID类型，给定局部ID和命名空间，返回对应的task_struct实例
							2. find_task_by_vpid：默认ID类型为PID类型，默认命名空间为current进程的命名空间，给定局部ID，返回对应的task_struct实例
							3. find_task_by_pid：默认ID类型为PID类型，默认命名空间为初始命名空间，给定全局ID，返回对应的task_struct实例
			3. 生成唯一的PID
				(1) 生成唯一的PID
					[1] 内核需要为PID生成在命名空间中唯一的数值。为跟踪已经分配和仍然可用的PID，每个命名空间都使用一个大的位图，其中每个PID由一个比特标识。分配PID就是将位图第一个
						值为0的比特位设置为1；反之，释放PID可通过将对应的比特位从1切换为0来实现。
				(2) 创建新的pid实例		
					[2] 在复制一个新进程时，也需要创建新的pid实例。进程可能在多个命名空间中都是可见的，对每个这样的命名空间，都需要生成一个局部PID。这是在alloc_pid中处理的。起始于
						建立进程的命名空间，一直到初始的全局命名空间，内核会为此间的每个命名空间分别创建一个局部PID。包含在struct pid中的所有upid都用重新生成的PID更新其数据。			
						每个upid实例都必须置于PID散列表中
		2.3.4 进程关系
			(1) 进程家族关系
				[1] 进程A分支形成进程B，则进程B是进程A的子进程。task_struct的children是链表表头，该链表中保存进程中的所有子进程
				[2] 进程A分支形成进程B1, B2...Bn，各个Bi进程之间的关系是兄弟关系。task_struct的sibling用于将兄弟进程连接起来，并连接在父进程的children表头上
	2.4 进程管理相关的系统调用
		2.4.1 进程复制
			(1) 进程复制相关的系统调用
				[1] fork是重量级调用，它创建了父进程的一个完整副本，然后作为子进程执行。Linux使用了写时复制技术降低了复制工作量
				[2] vfork类似与fork，但是它不创建父进程的副本。相反，父子进程之间共享数据，vofrk主要用于子进程形成后立即执行execve系统调用加载新进程的场景，内核保证在子进程
					退出或者开始新程序之前，父进程处于堵塞状态。引入了写时复制机制后，vfork已经不在具有速度优势，应避免使用它
				[3] clone产生线程，可以对父子进程之间的共享、复制进行精确控制
			1. 写时复制
				(1) 写时复制技术背景
					[1] 内核使用写时复制技术，以防止fork执行时将父进程的所有数据复制到子进程。该技术利用了下述事实：进程通常只使用了内存页的一小部分；在调用fork时，内核通常
						对父进程的每个内存页，都为子进程创建一个相同的副本。这会有两种不好的负面效应：(1) 使用了太多内存 (2) 内存页复制耗费时间
					[2] 如果子进程在进程复制之后立即调用exec系统调用加载新程序，那么负面效应会更严重。这实际意味着内核此前做的复制操作都是多余的，因此子进程地址空间会重新初
						始化，复制的数据不再需要了
				(2) 写时复制实现原理
					[1] 使用写时复制(COW)技术，内核可以不复制父进程的地址空间，而只复制其页表。fork之后，父进程和子进程的地址空间指向同样的物理内存页
					[2] 父子进程不允许修改彼此的页，这也是两个进程将页表对页表标记为只读访问的原因。加入两个进程只能读取内存页，那么两个进程的数据共享就不是问题
					[3] 假如有一个进程试图向复制的页写入，处理器会向内核报告访问错误（也被称作缺页异常，虚拟地址到物理地址的切换、页表项权限检查都是处理器的MMU做的，所以
						出现问题后，处理器会向内核报告缺页异常错误，内核会跳转到提前设置好的错误处理代码位置）。内核然后查看额外的内存管理数据结构，检查该页是否是可以
						用读写模式访问，还是只能以只读模式访问。如果是后者，则必须向进程报告段错误（即试图修改只读内存页）；如果页表项将一个页标记为“只读”，但通常情况下
						该页应该是可写的，内核可根据此条件判断该页实际上是COW页。因此内核会创建该页专用于当前进程的副本，当然也可以用于写操作
					[4] 我有一个疑问，内核会不会修改另一个进程的页表，从而将已经发生过写时复制的内存页标记为可写？如果是的话，内核是如何找到“另一个进程”的呢？或者在子进程
						调用exec加载新程序后，是否会将父进程的页表的某些项恢复为可写状态？还是说同样延后处理，父进程修改内存页时发现另一个进程不在共享该页了，然后顺便
						将页表项修改了。总是关于写时复制还有很多疑问，而对于这些疑问，我都能提出很多可行的解决方案，作为一名内核开发者，我必须能找到所有可能的解决方案，
						并且有能够实现这些方案的能力，以及选出最佳方案的判断力
					[5] COW机制时的内核可以尽可能延迟内存页的复制，更重要的是，很多情况下不需要复制。这节省了大量时间
			2. 执行系统调用
				(1) do_fork参数介绍
					[1] 体系结构相关的sys_fork、sys_vfork、sys_clone都会调用体系结构无关的do_fork函数
					[2] clone_flag是一个标志集合，用来指定控制复制过程的一些属性
					[3] stack_start是用户状态下栈的起始地址，加入该参数的目的是为了创建线程，因此线程之间虽然共享地址空间，但是栈却不在同一个位置
					[4] regs是一个指向寄存器集合的指针
					[5] stack_size是用户状态下栈的大小
					[6] parent_tidptr和child_tidptr是指向用户空间地址的两个指针，分别指向父子进程的PID
				(2) sys_fork、sys_vfork、sys_clone函数介绍
					[1] sys_fork使用的参数集合是(SIGCHLD, regs.esp, &regs, 0, NULL, NULL)，只使用了一个SIGCHLD标志，这意味着在子进程终止后发送SIGCHILD信号到
						父进程；父子进程使用相同的用户栈地址
					[2] sys_vfork相较于sys_fork增加了几个标志，即CLONE_VFORK和CLONE_VM，具体的工作由do_fork处理
					[3] sys_clone使用的复制标志不是静态的，而是可以通过各个寄存器参数传递到系统调用。另外，也不再复制父进程的栈，而是可以指定新的栈地址。另外还指定了用户
						空间的两个指针，parent_tidptr和child_tidptr，用于与线程库通信
			3. do_fork的实现
				(1) do_fork函数的前半流程
					[1] do_fork函数从copy_process开始，后者执行生成新进程的实际工作，并根据指定的标志选择重用还是复制父进程数据
					[2] 由于fork需要返回新进程的PID，因此必须获得PID。注意返回的PID必须是父进程命名空间中的ID数值，因为返回的子进程PID是给父进程看的
					[3] 如果要使用Ptrace监控新的进程，那么在创建新进程后会立即向其发送SIGSTOP信号，以便附接的调试其检查其数据
				(2) do_fork函数唤醒新进程
					[1] 子进程使用wake_up_new_task唤醒，换言之，就是将其task_struct添加到调度器队列。调度器也有机会对新启动的进程给予特别处理，这可以实现一种机制以便
						新进程有较高的机率尽快开始运行
					[2] 如果子进程在父进程之前运行，则可以大大减少复制内存页的工作量，尤其是子进程在fork之后立即执行exec调用的情况下。将进程排到调度器数据结构中并不意味着
						该子进程可以立即执行，而是此时调度器可以选择它运行。但调度器还是会特别处理，使其有更高的机率尽快运行
				(3) do_fork函数针对vfork的处理
					[1] 如果使用vfork机制，必须启用子进程的完成机制。子进程task_struct的vfork_done成员用于该目的，借助于wait_for_completion函数，父进程在该变量上
						进入睡眠状态，直至子进程退出。子进程退出或者使用execve加载新程序后，内核自动调用complete(vfork_done)，这会唤醒所有因该变量睡眠的进程。其
						时就是将睡眠进程的task_struct放入一个队列中，唤醒时就将这些task_struct放进调度器队列，和事件驱动机制很像
					[2] 通过这种方法，内核可以确保使用vfork生成的子进程的父进程一直处于不活动状态，知道子进程退出或者加载新程序。父进程的临时睡眠状态，也确保了两个进程不会
						彼此干扰或操作对方的地址空间。但这样说真的准确吗，除非用户进程在调用vfork后立即执行exec，否则子进程还是有可能修改共享地址空间的，而父进程对此
						一无所知。所以使用vfork一定要小心，创建子进程后必须立即加载新程序，否则一不小心就会出现bug
			4. 复制进程
				(1) 复制标志自洽
					[1] 复制进程受到标志的控制，但内核要保证标志组合没有矛盾，想要捕获这种组合并不困难，直接检查即可
					[2] 如果标志组合自相矛盾，内核就会返回错误码。内核要求函数成功时返回指针，失败时返回错误码，但是C语言函数只能返回一个值，正常的话无法判断返回值是指针还是
						错误码。因此内核保留了地址空间中0~4KB的部分，该区域中没有任何有意义的信息，引用该范围内指针的会报错，内核重用该地址范围来编码错误码。这样函数成
						功时正常返回指针，函数失败时返回0~4KB范围内指针，调用者通过指针数值判断错误原因。ERR_PTR用于将错误数值编码为0~4KB范围内指针
				(2) 创建父进程task_struct副本
					[1] 内核建立了自洽的标志集合后，则用dup_task_struct创建父进程task_struct的副本，新进程的task_struct实例可以在任何空闲的内核内存位置分配
					[2] dup_task_struct后，父子进程实例只有一个成员不同：新进程分配了一个新的内核栈，即task_struct->stack。内核栈与thread_info保存在一个联合中，
						thread_info是线程所需的所有特定于处理器的底层信息
					[3] 在大多数体系结构上，都使用一或两个内存页保存thread_union实例。在IA-32上，两个内存页是默认配置，因此可用的内核栈长度略小于8KB，其中一部分被thread_info
						占据。配置选项4KSTACKS会将内核栈长度降低到4KB，即一个页面。如果系统中有很多进程在运行，这样做是有利的，每个进程都节省了一个页面；另一方面，对于经常
						趋向于使用过多栈空间的外部驱动程序来说，这可能导致问题。某些外部的二进制驱动程序习于向可用的栈空间乱塞数据，这会导致内核栈溢出，从而发生未知的错误
					[4] thread_info中存储了特定于体系结构的汇编代码需要访问的那部分进程数据：
						1. task是指向进程task_struct实例的指针，内核栈、thread_info、task_struct是一一对应的关系
						2. exec_domain用于实现执行区间，例如在AMD64的64bit模式下运行32bit程序
						3. flags保存特定于进程的标志，我们对其中两个感兴趣：
							(1) TIF_SIGPENDING代表进程有待决信号
							(2) TIF_NEED_RESCHED代表进程应该或者想要调度器选择另一个进程代替自己运行
						4. cpu说明进程正在其上执行的cpu编号
						5. preempt_count用于实现内核抢占的一个计数器。进入临界区加1，退出临界区减1，值为0时代表可以进行内核抢占
						6. addr_limit指定了进程可以使用的虚拟地址的上限，该限制适用于普通进程，不适用于内核线程
						7. restart_block用于实现信号机制
					[5] 在内核的某个特定组件使用了过多栈空间时，内核栈会溢出到thread_info部分，这可能会导致严重问题。因此内核提供可kstack_end函数，用于判断给出的地址是否位于栈
						的有效部分之内
					[6] dup_task_struct会复制父进程task_struct的thread_info内容，但stack则与新的thread_info位于同一内存区域，也就是新建的内核栈区域。dup_task_struct结束
						后，父子进程的task_struct除了栈指针之外是完全相同的
					[7] 所有体系结构都必须定义名为current和current_thread_info的宏或函数。current_thread_info用于获取当前进程的thread_info的地址，可以根据CPU栈指针确定，
						因为thread_info总是位于内核栈顶。每个进程都有自己的内核栈，内核栈中有thread_info实例，因此进程、内核栈、thread_info的映射是唯一的。current指定
						了当前进程task_struct实例的地址，可以使用current_thread_info来确定，即current_thread_info()->task
				(3) 检查用户创建进程数是否超限
					[1] 父子进程同属于一个用户，内核会检查当前用户的进程数是否超过允许的最大进程数目
					[2] 用户创建的进程数保存在user_struct->processes中。我有一个模型能够更加直观地理解用户命名空间：
						1. 进程属于用户，一个用户会有多个进程
						2. 一个用户可能存在于多个用户命名空间中，其进程也会分布在这些用户命名空间中。用户命名空间彼此独立，没有层级关系。用户在每个所属的命名空间中都有一个
							user_struct结构，用于统计当前用户命名空间的用户属性
						3. 每个用户命名空间都有一个独立的user_struct用于统计当前命名空间中root用户的属性
						4. 默认情况下，子进程和父进程属于同一个用户，并且属于同一个用户命名空间
						5. 可以想像这样一个表格，横轴是各个用户，纵轴是各个用户命名空间，表格中是进程，root用户比较特殊，上面所说的属性在表中也有体现：
							——————————————————————————————————————————————————————————————————————
							|		|      用户1      |      用户2      |      用户3       |
							|  用户命名空间1	|      进程1      |                 |                 |
							|  用户命名空间2	|     		 |	进程2       |                 |
							|  用户命名空间3	|  进程3, 进程4   |                 |      进程5       |
							——————————————————————————————————————————————————————————————————————
					[3] 如果用户在当前用户命名空间创建的进程数量超过限制，并且该用户不是当前用户命名空间的root用户，而且该用户也没有被分配特殊权限，满足上述条件则放弃创建进程。上面
						所说的“当前用户命名空间”指的是父进程用户命名空间，用户创建进程数量限制保存在task_struct->signal->rlim[RLIMIT_NPROC].rlim_cur		
				(4) 初始化调度器相关字段
					[1] 如果资源限制无法防止进程建立，则调用函数sched_fork，以便内核有机会对新进程进行设置
					[2] 本质上是初始化一些调度器相关的统计字段，如果有必要还会在各个CPU之间对可用的进程均衡一下，同时将进程的状态设置为TASK_RUNNING，由于新进程还没有创建完成，并且
						该进程也没有被加入到调度器队列，因此该状态是不真实的。但这样可以防止内核的其他部分试图将进程的状态从非运行改为运行，并在进程的设置彻底完成之前调度进程，
						这个防止干扰的思路非常巧妙
				(5) 根据标志复制或共享各个子系统资源
					[1] 接下来会调用诸多形如clone_xyz的例程，以便复制或共享特定的内核子系统的资源
					[2] 如果设置CLONE_XYZ置位，则两个进程会共享资源，对应资源的引用计数加一，如果父进程或子进程修改了资源，则两个进程都可以看到。如果CLONE_XYZ没有置位，则会为
						子进程创建资源一个副本，新的副本的引用计数为1，如果父进程或子进程修改了资源，变化不会传播到另一个进程
					[3] 下面是我们感兴趣的资源：
						1. 如果CLONE_SYSVSEM置位，则copy_semundo使用父进程的System V信号量
						2. 如果CLONE_FILES置位，则copy_files使用父进程的文件描述符；否则创建新的files实例，其中包含的信息与父进程相同，该信息的修改可以独立于原结构
						3. 如果CLONE_SIGHAND或CLONE_THREAD置位，则copy使用父进程的信号处理程序
						4. 如果CLONE_THREAD置位，则copy_signal与父进程共同使用信号处理中不特定于处理程序的部分
						5. 如果COPY_MM置位，则copy_mm让父子进程使用同一地址空间，即两个进程使用同一个mm_struct实例；如果COPY_MM没有置位，也不必复制父进程的整个地址空间，
							而是创建父进程页表的一个副本，使用写时复制机制，仅当其中一个进程写入页时，才会进行实际复制
						6. copy_namespace用于创建子进程的命名空间，如果CLONE_NEWxyz置位，则为子进程创建一个新的命名空间，否则父进程共享命名空间
						7. copy_thread是一个特定与体系结构的函数，它复制执行上下文中特定于体系结构的所有数据。其实就是填充task_struct->thread的各个成员。这是一个
							thread_struct类型的结构，其定义是体系结构相关的。它包含了所有寄存器（和其他信息），内核在进程之间切换时需要保存和恢复进程的内容，该结构可
							用于此
				(6) 处理对父子进程不同的各个成员
					[1] 新进程的线程组ID与分支进程（调用fork/clone）的进程相同
					[2] 线程的父进程是分支进程的父进程，非线程的普通进程使用CLONE_PARENT触发同样的行为
					[3] 普通进程的线程组长是其本身，对线程来说，其组长是当前进程的组长
					[4] 将子进程的children链表连接到父进程的sibling链表上
					[5] 如果新进程的tgid等于pid，即该进程是线程组的组长，则其要将新进程归入到ID数据结构体系中：
						1. 如果置位了CLONE_NEWPID创建一个新的PID命名空间，那么必须要由新进程承担该命名空间init进程的角色
						2. 新进程必须被加到当前进程组和会话，首先设置全局SID和PGID，然后建立task_struct和pid实例的双向连接
					[6] 最后将PID本身加到ID数据结构的体系中，创建新进程的工作就此完成
			5. 创建线程时的特别问题
				(1) 在Linux内核中，线程和一般进程之间的差别不是那么刚性，本质上都是task_struct，在调度器眼中都是调度实体，最明显的差别就是线程与分支进程共享地址空间，而进程的地址空间是
					独立于分支进程的。还有一些进程关系、信号、ID、共享策略方面的不同，但没有太大区别
				(2) 用户线程库在实现多线程时，需要用到几个特殊的标志：
					1. CLONE_PARENT_SETTID将生成线程的PID复制到clone调用指定的parent_tidptr地址，进程复制时会执行此操作，告知用户空间线程正在创建
					2. CLONE_CHILD_SETTID将clone调用指定的用户空间指针child_tidptr保存在task_struct->set_child_tid中，进程第一次执行时，会将进程PID复制到该地址，告知用户
						空间线程创建完毕，并开始执行
					3. CLONE_CHILD_CLEARTID将clone调用指定的用户空间指针child_tidptr保存在task_struct->clear_child_tid中，进程终止时，向该地址写入0，告知用户空间线程已经
						终止
				(3) 线程终止时还会调用sys_futex函数，这是一个快速的用户空间互斥量，用户唤醒等待线程结束事件的进程
				(4) 上述标志可用于从用户空间检测线程的产生和销毁。CLONE_PARENT_SETTID和CLONE_CHILD_SETTID用于检测线程的生成，CLONE_CHILD_CLEARTID用于检测线程的终止。在多处理器
					系统上可以检测真正地并行执行
		2.4.2 内核线程
			(1) 内核线程的概念
				[1] 内核线程就是内核本身启动的线程，其实就是将内核函数委托给独立的进程，并与系统中其他进程并行执行，内核线程常被称为守护进程
				[2] 内核线程用于执行下列任务：
					1. 周期性地将修改的页与页来源块设备同步（如mmap文件映射）
					2. 如果内存页很少使用，则写入交换区
					3. 管理延时动作（如工作队列）
					4. 实现文件系统的任务日志
				[3] 内核线程分为以下两类：
					1. 线程启动后一直等待，知道内核请求线程执行某一特定操作
					2. 线程启动后按周期性间隔运行，检测特定资源的使用，在用量超出或低于预制的限制值时采取行动。内核使用这类线程用于连续监测任务
			(2) 创建内核线程
				[1] 内核使用kernel_thread启动一个新的内核线程，该函数有三个参数，即fn, arg, flags，产生的内核线程将执行fn执行的函数，而用arg指定的参数将自动传递给该函数，flag指定了
					CLONE标志。内核线程也是进程，Linux创建新的进程的唯一方法就是do_fork，因此要使用CLONE标志
				[2] kernel_thread函数首先会构建一个pt_regs的实例，对其中的寄存器指定适当的值。接下来调用do_fork函数创建新进程
			(3) 内核线程的惰性TLB处理
				[1] 内核线程相较于普通进程，有两个特别之处：
					1. 内核线程在内核态执行，而不是用户态
					2. 内核线程只能访问内核空间，而不能访问用户空间
				[2] 计算机的虚拟地址被分成两个部分，底部可以由用户程序访问，上部则专供内核使用。进程的虚拟地址的用户空间部分由task_struct->mm指定。每当内核执行上下文切换时，虚拟地址空间
					的用户层部分都会切换，并清空TLB缓存，以便与当前运行的进程匹配
				[3] 内核线程不与任何特定的用户层程序相关，内核并不需要倒虚拟地址空间的用户层部分，保留原设置即可。由于内核线程之前可能是任何进程在运行，因此用户层的内容本质上是随机的，内核
					线程绝不能修改其内容。为强调用户层不能访问，将mm设置为空指针。但内核需要知道用户空间当前包含了什么，所以使用task_staruct->active_mm描述当前的用户层。也就是说，
					mm指向的是进程的用户层，active_mm指向的是当前的用户层，对于普通进程来说，mm和active_mm是相同的
				[4] 内核根据如上特性实现惰性TLB处理，加入内核线程之后运行的进程和之前是同一个，那么就不用更换页表，TLB中的内容依然有效。否则要更换页表，并且清除TLB缓存。TLB缓存就是用于
					提升访存速度的，清除了缓存必定会导致新进程访存速度降低，因此使用惰性TLB机制毫无疑问可以提升系统性能，线程切换比进程切换更轻量也是这个原因
			(4) 内核线程实现函数
				[1] daemoniza用于将新创建的进程变为内核线程：
					1. 释放父进程的所有资源（用户层、文件描述符等），因为内核线程会运行到系统关机为止，因此必须清理其多余资源。而且内核线程只访问内核空间，甚至都用不到这些资源
					2. 阻塞信号的接受
					3. 将init进程作为内核线程的父进程
				[2] 内核提供了更便捷的函数，使用kthread_create创建内核线程，然后用wake_up_process唤醒它；使用kthread_run创建内核线程并立即唤醒它；kthread_create_cpu创建内核线程，
					并将它绑定到特定的CPU
				[3] 内核线程会出现在系统进程列表中，但在ps的输出中由方括号包围，以便与普通进程区分
		2.4.3 启动新程序
			(1) 通过新代码替换现存程序，即可启动新程序。Linux提供的execve系统调用可用于该目的
			1. evecve的实现
				(1) do_execve参数介绍
					[1] 该系统调用的入口是体系结构相关的sys_execve函数，该函数很快将其工作委托给体系结构无关的do_execve函数。对于后者，参数传递了可执行文件的文件名、寄存器集合、
						还传递了指向程序参数和环境的指针，这两个指针分别指向用户空间的两个指针数组
				(2) do_execve的打开可执行文件和bprm_init流程
					[1] 首先打开要执行的文件，内核找到相关的inode并生成一个文件描述符，用于寻址该文件
					[2] bprm_init接下来处理若干管理性任务：mm_alloc生成一个新的mm_struct实例来管理进程地址空间，init_new_context是一个特定于体系结构的函数，用于初始化该实例，
						而__bprm_mm_init则建立初始的栈
				(3) do_execve的prepare_binprm流程
					[1] 新进程的各个参数，包括euid, egid, 参数列表, 环境， 文件名等，合并成一个类型位linux_binprm的结构
					[2] prepare_binprm用于提供一些父进程相关的值（特别是euid和egid），剩余的数据则直接复制到linux_binprm实例中，该函数该维护了SUID和SGID的处理
					[3] uid和gid是进程所属的用户ID和用户组ID，在进程创建时就已经确定了。euid和egid时进程的有效用户ID和有效用户组ID，文件都是有权限标识的，分别对应创建文件的用户，
						同一个用户组的用户，其他用户对该文件的读、写和执行的权限。而euid和egid就是进程用来权限检查的有效ID，默认情况下它们与uid和gid相同，但是某些特殊情况下
						由可能不相同。比如一个只有root用户才能执行的文件，设置了SUID标志，那么属于其他用户的进程执行该文件是就会将自己的euid更新为该文件所属的用户ID，也就是
						root用户ID，这样一个普通用户就能够执行root用户的文件了，由于文件的内容是固定的，因此不用担心安全问题。SGID也是同理，只不过是将进程egid更新为文件所属
						的用户组ID
				(4) do_execve的search_binary_handler流程
					[1] Linux支持可执行文件的各种不同组织格式，标准格式是ELF
					[2] 尽管ELF格式尽量设计得与体系结构无关，但也不意味着同一个ELF格式可执行文件可以在不同体系结构中运行，因为不同体系结构使用的二进制指令集不同，二进制格式只表示
						如何在可执行文件和内存中组织程序的各个部分
					[3] search_binary_handler查找一种适当的二进制格式，用于所要执行的特定文件。各种格式使用不同的特点来识别，通常是文件起始处的一个“魔数” 
					[4] 二进制格式处理程序负责将新程序的数据加载到旧的地址空间中：
						1. 释放原进程使用的所有资源
						2. text段包含程序的可执行代码，start_code和end_code指定该段在地址空间中驻留的区域
						3. data段包含预先初始化的数据，start_data和end_data指定该段在地址空间中驻留的区域
						4. 堆用于动态内存分配，start_brk和brk指定了其边界
						5. 栈的位置由start_stack定义，栈自动向下增长
						6. 程序的参数和环境也映射到虚拟地址空间，分别位于arg_start和arg_end之间，以及env_start和env_end之间
						7. 设置进程指令指针和其他特定于体系结构的寄存器，以便在调度器选择该进程时开始执行其main函数					
			2. 解释二进制格式
				(1) linux_binfmt结构
					[1] Linux的所有二进制格式都通过linux_binfmt实例表示，不同二进制格式的linux_binfmt实例用链表连接起来
					[2] 每个linux_binfmt结构都包含以下三个函数：
						1. load_binary用于加载普通程序
						2. load_shlib用于加载共享库
						3. core_dump用于在程序错误的情况下输出内存转储。所谓内存转储就是在程序异常终止时，操作系统将进程的内存状态（包括代码、数据、堆栈、寄存器等信息）保存到
							磁盘文件中。执行路径是程序错误 -> CPU异常中断 -> 内核异常处理函数 -> 判定错误类型 -> 发送信号 -> 进程接收信号 -> 执行默认动作 -> 
							打开core文件 -> 写入内存数据（关键的代码、堆、栈信息，根据栈可确定调用路径） -> 写入寄存器和状态信息 -> 关闭core文件 -> 进程终止 ->
							释放进程资源，这就是内存转储的全过程。内存转储的核心目的是为调试提供“崩溃瞬间的快照”，生成的core文件可以通过gdb工具分析，定位错误原因。
							从很多地方都能看出来，内核在很努力的兼容gdb，因此一定要学好gdb这个工具
					[3] 使用register_binfmt注册新的二进制，其实就是将新的linux_binfmt结构连接到链表上
		2.4.4 退出进程
			(1) 退出进程机制
				[1] 所有进程都通过exit系统调用退出， 这使得内核有机会将进程使用的资源释放回系统，本质上是将各个资源的引用计数减1，如果引用计数为0则将内返还给内存管理模块
	2.5 调度器的实现
		(1) 在我看来，调度器有三大任务：
			[1] 当前任务要执行多长时间
			[2] 如何选择下一个任务
			[3] 执行上下文切换
                2.5.1 概观
			(1) 调度器概念
				[1] 内核必须提供一种方法，在各个进程之间尽可能公平地共享CPU时间，而同时又要考虑不同的任务优先级
				[2] 调度器的一般原理是，按所能分配的计算能力，向系统中的每个进程提供最大的公平性。或者从另一个角度来说，它试图确保进程没有被亏待
		2.5.2 数据结构
                	1. task_struct 的成员
                		(1) task_struct的调度器相关成员
					[1] 并非系统上的所有进程都同样重要，为确定进程的重要性，我们给进程增加了相对优先级属性
					[2] static_prio表示进程的静态优先级，静态优先级是进程启动时分配的优先级，他可以用nice和sched_setscheduler系统调用修改，否则它将保持恒定
					[3] norlmal_prio是根据静态优先级和调度策略计算出的优先级，即使普通进程和实时进程拥有相同的静态优先级，其普通优先级也是不同的。进程分支时，子进程会继承普通优先级
					[4] prio是调度期真正考虑的优先级，因为进程有可能临时调整优先级，而这种调整不能影响普通和静态优先级，因此加入了prio成员
					[5] rt_priority表示实时进程的优先级，最低的实时优先级是0，最高的是99，值越大优先级越高，与nice相反
					[6] sched_class表示该进程所属的调度类
					[7] entity是调度实体，调度器不仅可以调度进程。还可以用于实现组调度：可用的CPU时间可以首先在一般的进程组之间分配，接下来的时间在组内再次分配。这种一般性要求调度器不直接操作进程，
						而是操作可调度实体，可调度实体由嵌入到结构的sched_entity实例表示
					[8] policy保存了进程的调度策略，即使同属于一个调度器类，针对他们也有着不同的调度策略，Linux支持5个可能的值：
						1. SCHED_NORMAL用于普通进程，他们通过完全公平调度器处理
						2. SCHED_BATCH用于非交互、CPU密集型的批处理程序，也是由完全公平调度器处理。该类进程不会抢占CFS调度器上的进程，因此不会干扰交互式进程
						3. SCHED_IDIE进程的相对权重总是最小的，重要性比较低。虽然名称是SCHED_IDIE，但SCHED_IDIE不负责调度空闲进程，关于空闲进程内核提供单独的机制实现
						3. SCHED_RR和SCHED_FIFO用于实现软实时进程，他们由实时调度器类处理，SCHED_RR实现了一种循环方法，SCHED_FIFO实现了一种先进先出方法，rt_policy用于判断进程的调度策略是否是实时类
					[9] cpus_allowed是一个位掩码，用于表示进程允许在哪些CPU上运行，就是CPU亲和性
					[10] run_list和time_slice用于循环实时调度器，run_list是就绪队列的链表节点，time_slice进程可用的CPU剩余时间
			2. 调度器类
				(1) 调度器类的功能和结构
					[1] 调度器类是通过调度器和各个调度方法之间的关联。调度器类由若干个函数指针组成，通过函数指针访问对应调度方法。这使得通用调度器可以兼容多种调度方法，而无需了解它们的原理
					[2] 每个调度器类都对应一个sched_class的实例，所有的sched_class实例都被连接在一个链表上。他们是上下层级关系，即链表前面的调度器类优先级比后面的高。该链表在编译期就已经确定，不能在运行时更改
				(2) 调度器类的成员
					[1] void (*enqueue_task) (struct rq *rq, struct task_struct *p, int wakeup);
						enqueue_task向就绪队列添加一个新进程，当进程由睡眠状态转为可运行状态时，即发生该操作
					[2] void (*dequeue_task) (struct rq *rq, struct task_struct *p, int sleep);
						dequeue_task将一个进程从就绪队列中去除，在进程从可运行状态转为不可运行状态时就会发生该操作；内核还有可能因为其他理由将进程从继续
 					[3] void (*yield_task) (struct rq *rq);
 						当进程自愿放弃对处理器的控制权时，会调用sched_yield
 					[4] void (*check_preempt_curr) (struct rq *rq, struct task_struct *p);
 						在必要的情况下，会调用check_preempt_curr，用一个新唤醒的进程来抢占当前进程，例如使用wake_up_new_task唤醒新进程时，会调用此函数
 					[5] struct task_struct * (*pick_next_task) (struct rq *rq);
 					    void (*put_prev_task) (struct rq *rq, struct task_struct *p);
 						pick_next_task用于选择下一个将要运行的进程，而put_prev_task则在用另一个进程代替当前运行的进程之前调用
 					[6] void (*set_curr_task) (struct rq *rq);
 						在进程的调度策略更新时，会调用set_curr_task
 					[7] void (*task_tick) (struct rq *rq, struct task_struct *p);
 						每次激活周期性调度器时，由周期调度器调用task_tick
 					[8] void (*task_new) (struct rq *rq, struct task_struct *p);
 						new_task用于建立fork系统调用与调度器的联系，每次创建新进程都会调用task_new
 				(4) 内核封装函数
 					[1] 标准函数activate_task和deactivate_task调用前述的函数，提供进程入队和离队操作，同时还维护了一些统计变量
 					[2] 内核还提供了一个便捷函数check_preempt_curr，调用与给定进程相关的check_preempt_curr
 				(5) 调度策略与调度器类的映射
 					[1] 用户没办法直接设置进程的调度器类，一般情况下，子进程会继承父进程的调度器类
 					[2] 用户可以设置进程的调度策略，内核会根据调度策略选择其映射的调度器类	
 			3. 就绪队列
 				(1) 就绪队列介绍
 					[1] 核心调度器用于管理活动进程的主要数据结构称之为就绪队列
 					[2] 各个CPU都有自身的就绪队列，各个活动进程只出现在一个就绪队列中，在多个CPU上运行同一个进程是不可能的
 				(2) 就绪队列数据结构
 					[1] nr_running指定了队列上可运行进程的数目，不考虑其优先级或调度类
 					[2] load提供了就绪队列当前负荷的度量，就绪队列的负荷与队列上当前活动进程的数量成正比，其中的各个进程又有优先级作为权重。每个就绪队列的虚拟时钟即基于该信息
 					[3] cpu_load用于跟踪此前的负荷状态
 					[4] cfs和rt对应嵌入的子就绪队列，分别对应完全公平调度器和实时调度器
 					[5] curr指向当前运行进程的task_struct实例
 					[6] idle指向idle进程的task_struct实例，改进程在无其他可运行进程时运行
 					[7] clock和prev_raw_clock用于实现就绪队列自身的时钟，每次调用周期调度器时，都会更新clock的值。另外内核还提供了update_rq_clock函数，可在操作就绪队列的调度器中多处调用
 					[8] 系统中的所有就绪队列都在runqueues数组中，该数组的每个元素都对应于系统中的一个CPU
 			4. 调度实体
 				(1) 调度实体的概念
 					[1] 由于调度器可以操作比进程更一般的实体，因此需要一个适当的数据结构来描述该实体
 				(2) 调度实体数据结构
 					[1] load指定了权重，决定了各个实体占队列总负荷的比例。计算负荷权重是调度器的一项重任，因为CFS所需的虚拟时钟速度最终依赖于负荷
 					[2] rb_node是标准的树结点，是的实体可以在红黑树上排序
 					[3] on_rq表示当前实体是否在就绪队列上接受调度
 					[4] 在进程运行时，我们需要记录消耗的CPU时间，用于完全公平调度器。sum_exec_runtime即用于该目的。跟踪运行时间是由update_curr不断累积完成的，调度器中许多地方都会调用该函数，
 						比如新进程加入就绪队列，或者周期调度器中。每次调用时，会计算当前时间与exec_start的差值，exec_start更新到当前时间，差值被加到sum_exec_runtime。vruntime用于统计
 						进程的虚拟运行时间
 					[5] 当实体被撤销CPU时，当前的sum_exec_runtime被保存到prev_exec_runtime中。此后，在进程抢占中会用到该数据。另外，sum_exec_runtime不会置空，其会持续单调增长
 		2.5.3 处理优先级
 			1. 优先级的内核表示
 				(1) 优先级的范围
 					[1] 在用户空间内使用nice命令可以设置进程的优先级，这在内部会调用nice系统调用，nice值的范围是[-20,19]，值越低，优先级越高
 					[2] 内核使用一个简单的数据范围表示优先级，即[0,139]。[0,99]代表实时进程的优先级，nice值[-20,19]映射[100,139]的优先级，用于表示普通进程
 			2. 计算优先级
 				(1) 进程的三个优先级
 					[1] 进程有三个优先级，即静态优先级(task_struct->static_prio)、普通优先级(task_struct->normal_prio)、动态优先级(task_struct->prio)
 				(2) 计算普通优先级
 					[1] 内核调用normal_prio函数来初始化普通优先级，在函数内部会根据普通进程和实时进程做不同的运算
 					[2] 如果是普通进程，那么普通优先级与静态优先级相同
 					[3] 如果通过调度策略判断出是实时进程，那么就会根据rt_priority计算普通优先级，因其需要适配内核优先级越高值越低的特定，所以计算公式是MAX_RT_PRIO - 1 - p->rt_priority
 					[4] 也就是说，普通进程的normal_prio依赖于static_prio，实时进程的normal_prio依赖于rt_priority，根据policy即调度策略判断是否是实时进程
 				(3) 计算动态优先级
 					[1] 内核调用effective_prio计算动态优先级，函数内要考虑要因实时互斥量而临时提高优先级的普通进程
 					[2] 首先调用normal_prio计算普通优先级
 					[3] 然后根据当前动态优先级(p->prio)进行判断，如果其不在实时进程范围内，有两种情况，普通进程或者未设置动态优先级的实时进程，直接返回普通优先级
 					[4] 如果p->prio在实时进程范围内，说明有可能就是实时进程或者是因实时互斥量临时提高优先级的普通进程，此时prio不宜更新，因此直接返回p->prio
 					[5] 原文中的说法：如果是实时进程或已经提高到实时优先级，则保持优先级不变。否则，返回普通优先级
 				(4) 对各种类型进程计算优先级
 								static_prio		normal_prio		prio
 					[1] 非实时进程		static_prio		static_prio		static_prio
 					[2] 优先级提高的非实时进程	static_prio		static_prio		prio不变
 					[3] 实时进程		static_prio	  MAX_RT_PRIO-1-rt_priority	prio不变
 					[4] 进程是否是实时进程，取决于policy。进程的优先级，取决于static_prio或rt_priority
 				(5) 优先级的更新与继承
 					[1] 在新建进程用wake_up_new_task唤醒新进程时，或者执行nice系统调用更新优先级时，都会使用上述方法更新p->prio
 					[2] 子进程的静态优先级和普通优先级继承自父进程，而子进程的动态优先级则设置为父进程的普通优先级，这样可以避免父进程的优先级提高传播到子进程
 			3. 计算负荷权重
 				(1) load_weight数据结构
 					[1] 设置优先级的最终目的还是分配时间，光靠优先级数值很难做到合理分配。因此引入了负载权重的概念，调度器根据进程所占就绪队列的负载权重比例来分配时间
 					[2] load_weight结构中有两个值，一个是负荷权重，另一个是负荷权重的倒数。sched_entity内嵌一个load_weight结构，每个调度实体都有自己的负荷权重
 				(2) nice值与权重值的映射
 					[1] 内核的设计目标是，进程每降低一个nice值，则多获得10%的执行时间，每增加一个nice值，则减少10%的执行时间
 					[2] 这个10%是相对于自身的，比如一开始占50%的执行时间，那么增加一个nice值，则变为55%，再增加一个nice值，变为60.5%，以此类推
 					[3] nice值为0，也就是优先级为120时负载权重为1024，根据这个基准和前面的规则，可以计算出其他优先级的负载权重
 				(3) 就绪队列负荷权重
 					[1] 就绪队列也有自己的权重，为其中所包含的调度实体权重的和
 					[2] 在将进程放进就绪队列时，调用inc_nr_running可以更新rq->nr_running和rq->load，即活动进程数量和就绪队列总负载
 					[3] 内核使用三个函数，inc_nr_running、inc_load、update_load_add来实现上述功能，虽然逻辑很简单，但是却是很好的设计。每个函数对应一个逻辑，增强了可扩展性和代码可复用性，增强了可读性
 				(4) 学习内核“单逻辑函数”设计方法
 					[1] Linux的这种设计有什么好处？
 						1. 逻辑清晰，可读性极强，每个函数只承担 “最小且明确的逻辑单元”，每个函数的作用通过命名即可直观理解
 						2. 高内聚，低耦合：修改影响范围最小化
 						3. 复用性：原子逻辑可被多场景调用
 						4. 可测试性：单个逻辑单元易于验证，使用DFS回溯式的单元测试可以精准找到问题
 						5. 符合 “分层设计” 思想：从原子操作到业务组合，底层（原子操作，操作结构体成员），中层（模块内逻辑，就绪队列与进程load的关联），上层（业务逻辑，就绪队列状态更新的完整业务）
 					[2] 在平时的开发中如何实现这种优雅的设计呢？
 						1. 坚持 “单一职责”：一个函数只做一件事
 							(1) 判断标准：“能否用一句话清晰描述函数的作用？” 如果需要用 “并且”“同时” 连接多个动作，说明逻辑可以拆分
 						2. 按 “逻辑粒度” 分层，避免 “跨层操作”
 							(1) 函数的逻辑粒度应从 “原子操作” 到 “业务组合” 逐步提升，且上层函数只能调用下层函数，不能跳过中间层直接操作底层细节
 						3. 用 “命名” 强化逻辑边界
 							(1) 函数名必须准确反映其 “唯一职责”，避免模糊的命名（如do_something()、process_data()）
 						4. 先 “拆分” 再 “组合”，而非一开始就写大函数
 							(1) 开发时可以先梳理 “完成需求需要哪些步骤”，每个步骤作为一个小函数，最后用一个 “协调函数” 组合它们
 						5. 允许 “微小函数”，不必追求 “代码量”
 							(1) 不要觉得 “函数只有几行代码没必要单独抽出来”，内核中大量存在 “一行代码的函数”。这些函数的价值不在于代码量，而在于 “明确逻辑边界” 和 “提供复用点”
					[3] 这种 “单逻辑函数” 的设计，本质是通过 “明确每个代码单元的责任边界”，实现 “复杂系统的可控性”。Linux 内核作为千万行级代码的项目，正是依靠这种 “小函数、强内聚、低耦合” 的设计，
						让全球开发者能够协作维护——每个开发者只需理解自己涉及的几个小函数，就能安全地修改代码 。日常开发中，不必一开始就追求 “完美拆分”，但可以在写完代码后反问自己：“这个函数是否
						只做了一件事？能否拆出更原子的逻辑？” 长期坚持这种思维，代码会自然变得清晰、易维护，这就是 “优雅” 的本质
		2.5.4 核心调度器
			1. 周期性调度器
				(1) 学习内核的指针信任机制
					[1] 我们平时在进行C语言开发时，常会怀疑指针的合法性，恨不得每个指针都进行检查，这是对调用者和整个程序不信任的表现。Linux内核通过严谨的设计建立了指针信任，该检查的地方绝不省略，该信任的地方无需冗余
					[2] 内核是如何建立指针信任的呢？
						1. 定义严格的 invariants（不变式），并在全生命周期维护
							(1) task_struct->mm在用户进程中非空，在内核线程中为 NULL（明确区分场景，避免混淆）
							(2) struct file的f_op（文件操作集）永远非空（打开文件时由open系统调用确保，关闭后立即置空并释放）
							(3) 这些不变式由内核开发者共同遵守，任何代码修改都不能打破 —— 例如，若有代码试图将rq->curr设为 NULL，会被代码审查直接驳回
						2. 封装接口，隐藏实现，只暴露 “安全的访问方式”
							(1) 内核通过函数接口而非直接操作数据结构，确保指针获取的安全性
							(2) 不允许直接通过&per_cpu(rq, cpu)获取运行队列，必须通过cpu_rq(cpu)接口（内部会验证cpu的合法性）
							(3) 获取当前进程必须通过current宏（而非直接访问rq->curr），该宏在不同架构下有特殊处理（如 x86 通过fs寄存器偏移获取，确保非空）
							(4) 这些接口就像 “信任中介”——只要调用者正确使用接口，就无需担心返回空指针
						3. 初始化时 “预分配 + 强制清零”，避免未初始化指针
							(1) 内核中动态分配的结构体（如task_struct、rq）几乎都通过kzalloc（分配并清零）创建，而非kmalloc。这样未显式初始化的指针成员会被默认置为 NULL（避免随机值导致的野指针）
							(2) 结合 “初始化函数”（如task_struct_init），确保所有必要指针在结构体创建时就被正确赋值（非空的必须初始化，允许为空的明确标记）
						4. 调试工具 “兜底”，在开发阶段暴露问题
							(1) KASAN（Kernel Address Sanitizer）：通过插入内存守卫页，在空指针访问时立即触发崩溃并打印调用栈
							(2) BUG_ON 宏：在关键位置添加BUG_ON(ptr == NULL)，例如 “理论上不可能为空的指针” 若出现空值，直接触发内核恐慌（开发阶段快速暴露问题）
							(3) 静态分析工具：如sparse，会对 “可能返回 NULL 却未检查” 的代码报警，强制开发者处理
						4. 明确的 “函数契约”，通过文档和命名传递信任
							(1) 内核函数的注释会明确说明 “返回值是否可能为 NULL”
							(2) 若函数名含_get（如file_get），通常返回 “引用计数 + 1 的非空指针”
							(3) 若函数可能失败（如kmalloc），注释会明确 “失败时返回 NULL，调用者必须检查”
							(4) 若函数返回 “必然非空的指针”（如cpu_rq），注释会强调 “该函数永远返回有效指针”
					[3] 我们平时开发时如何建立指针信任，避免空指针问题呢？
						1. 明确 “边界检查” 与 “内部信任” 的分工
							(1) 边界处必须检查：所有来自 “外部” 的指针（如函数参数、用户输入、动态分配的返回值）必须检查
							(2) 内部逻辑可信任：同一模块内、经过边界检查后的指针，或由内部接口创建的指针（如自己写的create_xxx函数），可以不检查
						2. 用 “接口契约” 替代 “处处检查”
							(1) 通过文档、命名或宏定义，明确函数的 “输入输出约束”，让调用者和实现者达成共识
						3. 用 “初始化函数” 确保结构体指针就绪
							(1) 对包含多个指针成员的结构体，提供专用的 “创建 / 初始化函数”，确保所有指针在使用前被正确设置（非空的初始化，允许为空的显式置 NULL）
						4. 调试阶段 “过度检查”，发布阶段 “精简优化”
							(1) 开发 / 测试时：用宏启用严格检查（如assert(ptr != NULL)），快速暴露问题
						5. 借助工具提前发现问题
							(1) 编译期：启用-Wnonnull（GCC/Clang），对 “向非空参数传 NULL” 报警
							(2) 运行期：使用valgrind（检测野指针、空指针访问）、AddressSanitizer（编译时加入-fsanitize=address），在测试阶段暴露潜在问题
							(3) 静态分析：用clang-tidy等工具扫描 “可能的空指针解引用”，强制处理风险点
					[4] 内核不检查空指针，不是 “鲁莽”，而是因为它能通过设计确保指针 “必然非空”（可控）；应用层开发不必追求 “完全不检查”，但可以通过 “明确边界、定义契约、工具兜底”，让 “信任” 变得可控 
						—— 该检查的地方绝不省略，该信任的地方无需冗余，既保证安全，又保持代码简洁。最终，代码中的 “信任” 本质是对 “系统状态、接口契约、团队约定” 的可控性自信，而这种自信需要通过
						严谨的设计和工程实践逐步建立
				(2) 周期性调度器介绍
					[1] 周期性调度器在scheduler_tick中实现，如果系统正在活动中，内核会按照频率HZ自动调用该函数。如果没有进程等待调度，那么在计算机电力供应不足的情况下，也可以关闭该调度器减少电能消耗
					[2] scheduler_tick有两个任务，1. 管理内核中与整个系统与各个进程的调度相关的统计量；2. 激活负责当前进程调度类的周期性调度方法
				(3) 周期性调度器实现
					[1] 首先调用__update_rq_clock函数更新就绪队列的时钟，本质上是增加struct rq实例的时钟时间戳
					[2] 然后调用update_rq_load函数更新就绪队列的cpus_load数组，本质上就是将数组中先前存储的负荷值向后移动一个位置，将当前就绪队列的的负荷值记入数组第一个位置
					[3] 由于调度器的模块化结构，可以将主体的工作委托给特定调度器类的方法，即调用sched_class->task_tick
					[4] 在完全公平调度器中，task_tick函数会检查当前进程是否已经运行太长时间，以避免过长的延迟。这里是怎么判断是否运行太长时间的呢？CFS规定在一段时间内，就绪队列的每个进程都要至少运行一次，
						那么就根据每个进程的权重划分这段时间，如果进程运行超过这段时间则设置重调度标志，最终的效果就是调度器在尽量保证每个进程的vruntime是相同的，这里不再赘述，后面会详细解释
					[5] 如果进程应该被重新调度，则调度器会设置task_struct的TIF_NEED_RESCHED标志，以表示该请求，接下来内核会在主调度器中完成该请求。也就是说，如果当前进程因为各种原因需要被重新调度，
						那么只会设置TIF_NEED_RESCHED标志，真正的调度是在接下来的主调度器中执行的，也就是说检查调度和执行调度是分离的，为什么要这样做呢？因为调度必须在安全的上下文中执行，中断上下文、
						自旋锁深度不为0等都是不安全的上下文，比如周期性调度器可能就执行在软中断上下文中，不能直接进行调度，因此将调度请求和实际调度分开，让调度在安全、可控的环境下执行。主调度器是所有
						调度的起点，任何进程都是在主调度器函数中被调度，任何进程恢复运行也是在主调度器函数中继续执行，这里不再赘述，后面会详细解释
			2. 主调度器
				(1) 主调度器介绍
					[1] 在内核的很多地方，如果要将CPU分配给与当前活动进程不同的另一个进程，就会直接调用主调度器函数schedule
					[2] 从系统调用返回时，内核也会检查进程的TIF_NEED_RESCHED标志（内核很多地方都可能设置该标志，比如schedule_tick、进程主动让出CPU等），如果进程设置该标志则调用主调度器函数
					[3] 该函数假定当前活动进程一定会被另一个进程取代
				(2) __sched前缀
					[1] __sched前缀用于可能调用schedule的函数，包括schdule自身
					[2] 该前缀的目的在于，将相关的代码编译后，将目标文件放入一个特定的段中，即.sched.text
					[3] 这使得内核在显示栈转储或类似信息时，忽略所有与调度有关的调用。由于调度器函数不是普通代码流程的一部分，因此在栈转储等场景中是没有意义的
				(3) 主调度器的实现
					[1] 首先要意识到调度器在逻辑上是独立于进程的，即使调度器是从处于进程上下文的内核代码进入的，它在概念上也是和进程独立的，可以把它理解为一个特殊的“中断”，和进程上下文无关
					[2] 主调度器的目的就是用就绪队列中的一个进程取代当前活动的进程
					[3] 根据当前CPU ID，确定就绪队列，在prev中保存一个执行当前仍然活动的进程的task_struct指针
					[4] 类似于周期调度器，内核也利用该时机更新就绪队列时钟，然后清除当前活动进程task_struct的重调度标志TIF_NEED_RESCHED
					[5] 如果当前进程处于可中断睡眠状态但现在收到信号，那么它必须再次提升为可运行进程；否则，用相应调度类的方法使进程停止活动。举个例子，用户进程调用sleep函数，进程就会主动让出CPU，即设置
						TIF_NEED_RESCHED标志，进入可中断睡眠状态，这个状态是可以被信号唤醒的，所以在进入睡眠状态前就收到信号，进程仍然保留在就绪队列中并恢复可运行状态，这样可以提高信号响应速度，并且
						符合可中断睡眠可以被信号唤醒的逻辑，可以看出来内核在极尽优化效率，并且设计十分灵活大胆，真是艺高人胆大。除了以上的状态和进程被抢占的状态，其他所有的进程睡眠状态都要与就绪队列断开
						联系，脱离就绪队列的进程就意味着无法得到CPU，即睡眠状态。进入睡眠状态的进程通常绑定在某一链表中，等待某个事件或者内核将其唤醒
					[6] put_prev_task通知调度器类当前的进程将要被另一个进程代替，这提供了一个时机更新统计量
					[7] 调度器还必须选择下一个要运行的进程，该工作由pick_next_task负责
					[8] 如果选择了一个新进程，则调用context_switch执行底层上下文切换
					[9] 检测当前进程的重调度位是否设置，并跳转到主调度器起始位置，重新开始搜索一个新的进程。这段代码可能和之前的代码可能在不同的上下文执行，如果没有执行上下文切换，则直接执行这段代码；如果执行
						了上下文切换，当前进程会刚好在switch_to位置停止运行，直到下一次被调度器选择运行，它会刚在在此位置恢复运行。也就是说，所有进程调度都是在主调度器中完成的，所有进程也都是在主调度器中
						恢复的。由于prev不会指向正确的进程，因此通过current和tesk_thread_flag找到当前进程，如何理解这段话呢？调度器是独立于进程的，所以进程恢复运行后，prev指向的是上一个运行的进程，而
						不是当前进程，想要找到当前进程就要通过thread_info，current和tesk_thread_flag都是基于thread_info，thread_info基于CPU的内核栈寄存器
			3. 与fork的交互
				(1) sched_fork
					[1] 每当使用fork系统调用或其变体之一创建新进程时，调度器有机会通过sched_fork函数挂钩到该进程
					[2] 在单处理器系统上，该函数执行三个操作：初始化新进程与调度器相关的字段、建立数据结构(相当简单直接)、设置新进程的动态优先级
					[3] 父进程的普通优先级作为新进程的动态优先级，是为了避免父进程的优先级临时提高传播到子进程。如果动态优先级不在实时范围内，则进程总是从完全公平调度类开始
				(2) wake_up_new_task
					[1] 在使用wake_up_new_task唤醒新进程时，则是调度器和进程建立连接的第二个时机。内核会调用进程调度类的task_new函数，该函数会将新进程加入到相应调度类的就绪队列
			4. 上下文切换
				(1) 上下文切换介绍
					[1] 内核选择新进程后，必须处理多任务相关的细节，这些细节总称为上下文切换，上下文即执行环境。辅助函数context_switch是一个分配器，它会调用所需的特定于体系结构的方法
					[2] 上下文切换本质上通过调用两个体系结构相关的函数来完成：
						1. switch_mm：更换通过task_struct->mm描述的内存管理上下文，具体的工作是加载页表、刷出地址转换后备缓冲器(部分或全部)、向内存管理单元(MMU)提供新的信息
						2. switch_to：切换处理器寄存器内容和内核栈，用户空间在switch_mm中已经切换，所以不用切换用户栈。不用担心进程切换会影响进程用户态的运行状态，进程切换都是内核态的主调度器函数中进行的，	
							用户态信息保存在内核栈中，当进程被调度器再次选择运行并回到用户空间后，可以使用内核栈中的信息恢复运行状态，切换内核栈就是切换运行进程用户空间运行状态
				(2) switch_mm
					[1] switch_mm就是执行不同体系结构的加载页表、刷出地址转换后备缓冲器(部分或全部)、向内存管理单元(MMU)提供新的信息工作，但是有一个特殊情况，即内核线程
					[2] 内核线程没有对应的用户空间，可能在随即的进程地址空间上执行，因此其task_struct->mm = NULL，task_struct->active_mm设置为当前实际内存管理上下文，即上一个进程的active_mm
					[3] 如果下一个进程是内核线程，那么设置其active_mm为prev->active_mm，并且调用enter_lazy_tlb通知底层体系结构不需要更新地址空间的用户空间部分
					[4] 如果上一个进程是内核线程，则要设置prev->active_mm = NULL，因为内核线程的active_mm是随机的，其退出CPU后继续保留该信息是没有意义的
				(3) switch_to
					[1] 最后使用switch_to函数完成进程切换，该函数切换寄存器内容和内核栈
					[2] switch_to后面的代码要在进程被下一次选择运行是才会执行，finish_task_switch完成一些清理工作，使得能够正确的释放锁
				(4) switch_to的复杂之处
					[1] finish_task_switch的有趣之处在于，它的清理是针对之前运行的进程，而不是当前上下文的进程。把调度器理解为一个独立的模块就能够理解这段话
					[2] 如何找到之前运行的进程呢？通过switch_to宏实现，switch_to(prev, next, prev)通过三个参数传递两个变量，内部的调用格式是prev = switch_to(prev, next)
					[3] 也就是说当进程被调度器再次选择运行，switch_to这个宏就将上一个运行进程的task_struct指针赋值给prev，这样就找到了上一个运行进程，调用finish_task_switch对上一个进程做一些清理工作
					[4] 调用finish_task_switch的时机非常巧妙，它被barrier()准确的放在了switch_to的后面，含义就是成功切换到下一个进程后，再执行上一个进程的清理工作，这样是最稳妥的；如果在进程切换之前，
						就清理当前进程，这样会有一瞬间的空隙，从这里足以看到Linux内核的巧妙
				(5) 惰性FPU模式
					[1] 由于上下文切换对内核性能的影响举足轻重，所以内核使用了一种技巧减少其所需的CPU时间
					[2] CPU的浮点寄存器内容，除非有应用进程使用，否则不会保存；此外，除非应用程序需要，否则不会恢复，称之为惰性FPU技术
					[3] 举个例子，A进程使用浮点计算，B进程不使用，A进程切换到B进程时，浮点寄存器内容被保存到线程数据结构中，但由于B进程没有使用浮点运算，所以没有执行浮点寄存器的恢复；当B进程切换到A进程时，
						因为B进程没有使用浮点运算，所以没有执行浮点寄存器的保存，A进程也看到浮点寄存器的内容没有改变，因此也没有进行浮点寄存器的恢复，如果B进程进行了浮点运算，这个事件会报告给内核，并且
						切换到进程A时，会恢复之前的浮点寄存器内容
					[4] 只有需要的情况下，内核才会保存和恢复浮点寄存器内容，不会因为多余的操作浪费时间、
	2.6 完全公平调度器
		2.6.1 数据结构
			(1) cfs_rq
				[1] 完全公平调度器的主要数据结构就是其就绪队列struct cfs_rq
				[2] nr_running计算了队列上可运行进程的数目，load维护这这些进程的累积负荷值
				[3] min_vruntime跟踪记录队列上所有进程的最小虚拟执行时间，这个值是实现与就绪队列相关的虚拟时钟的基础
				[4] task_timeline是一个红黑树的根节点，用于在按时间排序的红黑树中管理所有进程，rb_leftmost指向红黑树中最左边的节点，即最需要被调度的进程，该进程也可以通过遍历红黑树得到，但是我们通常只对最左边的
					节点感兴趣，保存下来可以避免搜索红黑树的时间开销
				[5] curr指向当前执行进程的可调度实体
		2.6.2 CFS操作
			1. 虚拟时钟
				(1) 虚拟时钟介绍
					[1] 完全公平调度算法依赖于虚拟时钟，虚拟时钟可以根据现存的实际时钟和进程的负荷权重推算出来，所以与虚拟时钟相关的操作都在update_curr中执行，该函数在系统中不同地方调用，包括周期性调度器
				(2) update_curr实现
					[1] 首先该函数确定就绪队列的当前执行进程，然后获取主调度器就绪队列的实际时钟值，该值在每个调度周期都会更新
					[2] 然后计算与上次调用update_curr的时间差，并将其余的工作委托给__update_curr
					[3] __update_curr需要更新当前进程在CPU上运行的物理时间和虚拟时间，物理时间的更新比较简单，只需要将时间差加到先前统计的物理时间即可
					[4] 接下来更新虚拟时钟，设定对于nice值为0的进程，虚拟时间和物理时间是相等的，对于其他优先级的进程，则需要根据负荷权重衡定虚拟时间，计算公式是物理时间 * NICE_0_LOAD / curr->load.weight，
						优先级越高，nice值越低，负荷权重越高，虚拟时间增长的就慢；优先级越低，nice值越高，负荷权重越低，虚拟时间增长就慢
					[5] 接下来内核更新cfs_rq->min_vruntime，首先从当前进程的vruntime和红黑树最左边节点的vruntime中取最小值，然后与min_vruntime取一个最大值，保证min_vruntime是单调递增的
					[6] 为什么要计算min_vruntime呢？因此进程在红黑树中实际排序的键值就是 curr->vruntime - min_vruntime，键值越小的节点，在红黑树中的排序就更靠左，因此也会更快被调度。利用这个方法，内核实现
						了两个机制：
						1. 进程运行时，其vruntime稳定地增加，它在红黑树中总是向右移动的，优先级越高的进程虚拟时间增长越慢，向右移动的速度也就越慢，也就更容易被调度
						2. 如果进程进入睡眠，则其vruntime保持不变，因为每个就绪队列的min_vruntime单调递增，那么在进程醒来时，其在红黑树中的位置就更靠左
			2. 延迟跟踪
				(1) 调度延迟的概念
					[1] 内核有周期调度器时刻监控着进程的执行时间，如果进程执行了太长时间，就会设置进程的TIF_NEED_RESCHED重调度标志，后续主调度器执行调度操作。但是如何判断进程是否执行过长时间呢？这其实是进程在
						获得CPU时根据调度延迟推算出来的，调度延迟可以理解成一个周期，内核保证该周期内，每个可运行的进程都会至少运行一次
					[2] 内核有一个概念，即良好的调度延迟，即保证每个可运行进程都至少执行一次的时间间隔，由sysctl_sched_latency给出，它默认是20ms。也就是说20ms内，每个可运行的进程都至少运行一次
					[3] 内核还有一个参数，sched_nr_latency，控制在一个延迟周期内最大的可运行进程数目。如果当前可运行进程的数量超过该上限，则延迟周期即sysctl_sched_latency也会成比例的扩展
					[4] 如果可运行进程非常多，内核肯定不能保证在调度延迟内每个进程都会运行一次，要满足该条件的进程数量上限就是sched_nr_latency，一旦超过该上限，就要成比例的扩展调度延迟的长度
				(2) 调度延迟的计算
					[1] 在__sched_period中计算调度延迟，sysctl_sched_latency * (nr_running / sched_nr_latency)
					[2] 根据各个可运行进程的负荷权重，将一个延迟周期的时间在活动进程之间分配，period = __sched_period(); slice = period * (se->load.weight / cfs_rq->load.weight)
					[3] 计算各个可运行进程的虚拟时间，vtime = period * (NICE_0_LOAD / cfd_rq->load.weight)，vtime = period * (se->load.weight / cfs_rq->load.weight) * (NICE_0_LOAD / se->load.weight)
						也就是说分配给所有可运行进程的虚拟时间都是相同的
		2.6.3 队列操作
			(1) enqueue_task_fair
				[1] 该函数的参数除了cfs_rq，task_struct指针外，还有一个名为wakeup的参数，如果进程最近才被唤醒并转为可运行状态，wakeup就是1；如果此前就是可运行的，那么wakeup就是0
				[2] 函数的处理逻辑很简单，如果通过se->on_rq得知进程本来就在就绪队列上，则直接返回；否则，调用enqueue_entity函数，内核还会借助这个时机调用update_curr更新统计量，主要是更新当前进程vruntime
				[3] 在enqueue_entity函数中执行的逻辑也很简单，如果进程当前没在执行，那么直接调用__enqueue_entity将该进程加入到红黑树中
				[4] 这里有一个特殊情况，假如即将要加入就绪队列的进程是刚唤醒的进程，那么其vruntime会和cfs_rq->min_vruntime有很大差值，因为睡眠进程的vruntime不变，而cfs_rq->vruntime是单调递增的，这就导致新加入
					进程在红黑树中更靠左，因此内核设置其se->vruntime = max(cfs_rq->min_vruntime - sysctl_sched_latency, se->vruntime)，避免其长时间占用CPU，也避免不必要的补偿短时间睡眠进程
		2.6.4 选择下一个进程
			(1) pick_next_task_fair
				[1] CFS通过pick_next_task_fair获取下一个要执行的进程的调度实体
				[2] 如果就绪队列中没有待执行的进程，直接返回NULL；否则直接选择leftmost对应的最左边的节点，并用container_of机制获取对应的调度实体
				[3] 调用set_next_entity将该调度实体标记为可运行进程，主要工作是：如果当前进程在红黑树中，则将其移除，设置leftmost为当前最左边的进程，设置cfs_rq->curr，prev_sum_exec_runtime = 
					sum_exec_runtime，后者不会重置，其统计进程总执行时间，因此sum_exec_runtime-prev_sum_exec_runtime表示了进程在CPU上的执行时间，也就是进程在当前调度所执行的时间
		2.6.5 处理周期性调度器
			(1) entity_tick
				[1] 该函数首先调用update_curr更新统计量，主要是当前进程的物理时钟和虚拟时钟
				[2] 如果当前就绪队列上的进程少于两个，则直接返回，因为即使给当前进程设置了重调度标志，也没有进程能抢占它；否则，计算sum_exec_runtime-prev_sum_exec_runtime，即进程在当前调度所执行的时间是否大于
					分配的调度周期份额，如果超过，说明进程已经运行了太长时间，设置进程的重调度标志，后续主调度器会在合适的时机执行调度
		2.6.6 唤醒抢占 
			(1) check_preempt_wakeup
				[1] 当内核调用try_to_wake_up或wake_up_new_task时，内核调用check_preempt_wakeup检查被唤醒的进程是否可以抢占当前进程
				[2] 被唤醒的进程有可能不是由完全公平调度器处理，如果被唤醒的进程是实时进程，则直接抢占当前进程，因为实时进程总是会抢占CFS进程
				[3] 内核保证被抢占的进程必须已经执行了最小时间限额，由sysctl_sched_wakeup_granularity，默认是4ms。内核将这个值转换成被唤醒进程对应的虚拟时间，设为gran，如果被唤醒进程的虚拟时间+gran小于当前进程
					的虚拟时间，才会执行抢占。这里的设计思路是什么呢？进程可能是被某些事件或信号唤醒的，唤醒它就是为了尽快处理该事件，因此我们肯定是希望被唤醒的进程要尽快运行，但是如果事件发生的非常频繁，那么进程
					也会切换的非常频繁，这样大部分的时间都被浪费在上下文切换上了，加入一个时间缓冲有助于减轻这种情况
		2.6.7 处理新进程
			(1) task_new_fair
				[1] 当内核使用fork家族系统调用创建新进程时，主调度器会调用task_new_fair经普通进程挂钩到CFS调度器上
				[2] 首先利用这个时机调用update_curr更新统计量，然后调用place_entity函数，之前enqueue_entity_fair也调用过该函数，用于初始化新加入就绪队列进程vruntime。如果参数sysctl_sched_child_runs_first
					为1并且当前进程的vruntime大于父进程的vruntime，则交换二者的vruntime，目的是保证新创建的子进程优先于父进程运行，这样做是有意义的，尤其是在子进程马上会执行exec系统调用家在新程序时，父子进程
					的地址空间是写时复制的关系，如果父进程先运行，那么可能会发生写时复制，有额外的复制时间，这个复制是没必要的，因此子进程马上要加载新程序；如果子进程先运行，则其马上执行exec系统调用执行新进程，父子
					进程之间没有了写时复制的关系，后续父进程再运行就不会有额外的复制开销了
	2.7 实时调度类
		2.7.1 性质
			(1) 实时调度类性质
				[1] Linux支持两种实时调度类，有两种检测进程是否为实时进程的方法：1. 检查优先级 2. 检查调度策略
				[2] 实时进程的优先级比普通进程高，如果系统中有可运行的实时进程，则调度器总是会选择实时进程运行，除非有优先级更高的实时进程
				[3] 有两种实时调度策略：
					1. SCHED_RR：循环策略，同一优先级循环执行，只有高优先级任务执行完成，低优先级任务才能得到执行。在循环策略中，相同优先级的进程被放进对应优先级的队列，每个优先级都有对应的队列。对于优先级相同的进程
						来说，被选择运行时都会分配一个时间片，该值随着进程运行而减少，其为0时会发生调度，将该进程置于优先级队列末尾，队列的头部进程获得CPU
					2. SCHED_FIFO：先进先出策略，可以理解为依次执行，只有队列前面的进程执行完成，队列后面的进程才可以执行。如果进程永远不会结束，那就永远不会发生调度，很危险
		2.7.2 数据结构
			(1) 就绪队列
				[1] 实时调度类最核心的数据结构就是其就绪队列，用链表表示
				[2] 实时进程的优先级是0~99，总共100个优先级，因此就绪队列就是100个链表，相同优先级的实时进程保存在一个链表中
				[3] 就绪队列中还有一个bitmap位掩码，每个比特位对应一个链表，如果链表中包含了进程，那么对应的比特位。该位掩码的目的是可以用lowbit方法快速找到优先级最高的有活动进程的链表
				[4] 实时调度器中与update_curr对应的函数是update_curr_rt，update_curr主要统计进程的实际执行事件和虚拟执行时间，虚拟时间的目的是让不同优先级进程拥有不同的时间流速。实时进程不需要考虑这一点，它是同一优先级
					轮流执行，不同优先级依次执行，不是完全公平策略，而且不同的链表已经对应不同的优先级，因此不需要统计虚拟时间，只需要统计实际时间即可
		2.7.3 调度器操作
			(1) 入队和出队
				[1] 入队和出队操作十分简单，只要找到对应优先级的链表，然后将进程加入或移出链表即可，注意新加入的进程要加入到链表末尾
			(2) 选择下一个进程
				[1] 通过sched_find_first_bit函数找到位掩码中第一个置位的比特位，该函数的原理就是lowbit，更低的优先级数值实际代表着更高的优先级，所以该比特位对应的链表就是优先级最高的有活动进程的链表，取出该链表的第一个进程，
					并将se->exec_start设置为当前时间即可
			(3) 周期调度
				[1] SCHED_FIFO类型进程很好处理，它们可以运行任意长时间，而且执行结束后必须执行yield系统调用将控制权显式传递给下一个进程
				[2] SCHED_RR类型则减少其时间片，如果时间片为零，则将其重置为100 * HZ / 1000，也就是100ms，然后将该进程放置到对应优先级的链表末尾，设置重调度标志申请调度
			(4) 将普通进程转换为实时进程
				[1] 想要将普通进程转换为实时进程，必须使用sched_setscheduler系统调用
				[2] 该系统调用执行以下操作：将该进程从原有队列中移除，设置该进程的优先级和调度器类，重新激活进程，即将进程放入到实时调度器的就绪队列中
				[3] 注意只有root权限的进程才能通过sched_setscheduler系统调用将普通进程转换为实时进程，其他权限的进程执行该系统调用时会受到限制
					1. 无法切换调度器类
					2. 调度策略，可以将SCHED_NORMAL切换成SCHED_BATCH，或者反过来，但是不能切换成SCHED_FIFO
					3. 优先级，只有目标进程的UID或EUID和调用者的EUID相同时才能修改优先级，此外优先级只能降低，不能提高
	2.8 调度器增强
		2.8.1 SMP调度
			(1) CPU负荷必须尽可能公平地在所有的处理器上共享
			(2) 进程与系统中某些处理器的亲和性必须是可设置的
			(3) 内核必须能够将进程从一个CPU迁移到另一个
			1. 数据结构的扩展
				(1) 调度类新增了两个函数指针用来调度均衡和迁移进程，分别是load_balance和move_one_task
				(2) iter_move_one_task从最忙碌的就绪队列中移出一个进程，迁移到当前CPU的就绪队列；load_balance则从最忙的就绪队列中分配多个进程到当前CPU，但移动的负荷
					不能超过参数max_load_move
				(3) 周期调度器函数scheduler_tick函数每周期会调用trigger_load_balance函数，这会引发SCHEDULE_SOFTIRQ软中断。该中断确保在正确的时机执行run_rebalance_domains，
					该函数最终对当前CPU调用rebalance_domains，实现周期性地均衡就绪队列
					run_rebalance_domains 与 SCHEDULE_SOFTIRQ软中断 绑定
					定时器 -> scheduler_tick -> trigger_load_balance -> SCHEDULE_SOFTIRQ
					run_rebalance_domains -> rebalance_domains -> load_balance -> move_tasks -> (class->load_balance)
				(4) 就绪队列的数据结构也添加了一些成员用于辅助负载均衡和进程迁移，其中：
					cpu代表该就绪队列所属的处理器
					内核为每个就绪队列提供了一个迁移线程migration_thread，所有的迁移请求都保存在链表migration_queue中
					内核试图周期性地均衡就绪队列，但如果效果不佳就需要主动均衡，设置active_balance为非零值，push_cpu存储是哪个处理器发起的主动均衡请求
				(5) 物理上邻近或者共享高速缓存的CPU集群组织为调度域，该结构有很多参数可以控制调度域的负载均衡属性
				(6) load_balance函数首先执行find_busiest_queue找到工作量最大的就绪队列，如果最繁忙的队列至少有一个进程，则调用move_tasks将该队列中适当数目的进程迁移到当前队列,
					在move_task中调用特定于调度类的load_balance方法
				(7) 被迁移的进程的进程要满足以下的条件：
					目前没有运行或刚结束运行，因为迁移正在运行的进程抵消了高速缓存带来的好处
					根据CPU亲和性，被迁移的进程可以在目标进程运行
				(8) 如果均衡操作失败，那么将唤醒最忙的就绪队列的迁移进程，申请主动均衡
			2. 迁移线程
				(1) 迁移线程是一个执行migration_thread的内核线程。迁移线程有两个目的，一个用于完成发自调度器的迁移请求，另外一个用于实现主动均衡
				(2) 迁移线程主体是一个无限循环，无事可做时进入睡眠状态。首先检测是否需要主动均衡，如果需要则调用active_load_balance函数，该函数试图从当前就绪队列移出一个进程，
					且移至发起主动均衡请求的CPU的调度队列，它分别调用不同调度类的load_balance函数
				(3) 完成主动均衡负载之后，迁移线程会检测migrate_req是否有迁移请求，如果有则直接移出所要求的进程，如果没有则设置重调度标志让出CPU
			3. 核心调度器的改变
				(1) 利用exec系统调用启动新进程时迁移进程，因为此时进程尚未执行，此时迁移不会带来对高速缓存的负面影响。挂钩sched_exec函数，先使用sched_balance_self挑选当前
					负荷最少的CPU，然后用sched_migrate_task向迁移线程发送一个迁移请求
				(2) 完全公平调度器的调度粒度与CPU的数目是成比例的
		2.8.2 调度域和控制组
			(1) 调度器处理的是调度实体，因此可以实现组调度，可以先在组之间保证公平，然后组中的所有进程之间保证公平
			(2) sched_entity中加入了成员parent实现组调度的层次结构，所有调度类相关的操作，都必须考虑到调度实体的这种结构
		2.8.3 内核抢占和低延迟相关工作
			1. 内核抢占
				关于内核抢占的理解：
					(1) 抢占机制：在符合适当条件的情况下，比如进程的执行时间超过调度延迟，会设置重调度标志，然后在合适的时机执行调度。这个合适的时机就只有schedule主调度函数，
						只有在主调度函数中才能发生上下文切换。从内核状态进入用户状态时会调用主调度函数，但是这还远远不够，我们还想在内核执行过程中也可以被抢占。
					(2) 首先什么情况不能被抢占，当内核访问共享资源时不能抢占，也就是临界区。内核进入临界区时会申请自旋锁，内核退出临界区时会释放自旋锁，如果在临界区内发生抢占
						可能导致死锁或冲突问题。但是又不能搞一个定时器去检测是否可以抢占，因为那样效率太低。因此最理想的方案就是在释放锁时检测是否可以抢占，并调用主调度函数。
					(3) 每个处理器都有一个抢占计数器，每次进入临界区将这个计数器加一，每次退出临界区将这个计数器减一，计数器为零代表可以抢占，进入主调度函数。
				(1) 在返回用户状态之前或者内核某些指定的点上，内核会调用主调度器函数进行调度，在其他情况下内核的执行流不会中断。但如果内核处于耗时相对较长的操作中，其他进程无法运行，
					则会造成卡顿的现象，因此需要在内核执行过程中允许更高优先级的进程抢占，也就是要在合适的时机在内核代码中穿插主调度函数。
				(2) 内核在临界区执行时是不允许抢占的，因为进入临界区时会申请自旋锁，如果此时抢占可能导致死锁或者共享资源冲突。如果允许内核抢占，那么即使是单核处理器也可能发生这种问题。
					因此内核代码的执行流就可以分为可抢占区和不可抢占区。
				(3) 每个线程都有一个thread_info结构在内核栈的顶端，其中有一个preempt_count变量，如果该变量为0，代表该进程允许内核抢占，如果大于0，代表该进程不允许内核抢占。每次进入
					临界区都对该计数器加1，每次退出临界区都对该计数器减1，临界区可能嵌套
				(4) 有很多例程可用于抢占处理：
					[1] preempt_disable 通过调用inc_preempt_count停用抢占
					[2] preempt_enable_no_resched 通过调用dec_preempt_count启用抢占，但是不进行调度检查
					[3] preempt_check_resched 会检测重调度标志，如有必要则进行调度
					[3] preempt_enable 通过调用 [2] barrier() [3] 启用抢占，并进行调度检查
				(5) 在申请自旋锁等过程都会自动调用内核抢占例程，但是某些特殊情况还需要手动调用preempt_disbale禁用抢占
				(6) 内核抢占需要满足两个条件：
					(1) 进程设置了TIF_NEED_RESCHED重调度标志
					(2) 内核当前不处于临界区并且没有停用硬件中断（停用硬件中断的目的就是一次性完成相关处理，因此不应该进行重调度）
				(7) 在进入主调度器函数之前，给抢占计数器设置PREEMPT_ACTIVE标志，表示进入内核抢占，用于某些情况的特判，比如不能重复进入内核抢占，设置内核抢占标志会跳过某些操作，从而
					保证尽可能快速地选择下一个进程，而无需停止当前进程的活动
				(8) 另一种激活内核抢占的方法是中断返回到内核态时会调用preempt_schedule_irq来检测调度条件，如有必要进则行调度。该函数内禁止硬件中断从而避免递归调用
			2. 低延迟
				(1) 即使没有内核抢占，内核也会尽量提供良好的延迟时间。方法是在耗时较长的循环或处理中插入cond_resched函数，次函数会检查调度，如有必要则进行调度。这样即使没有启用内核
					抢占或者没有退出临界区，也能保证较高的响应速度
	2.9 小结
		(1) Linux是一个多用户、多任务操作系统，因而必须管理来自多个用户的多个进程。进程是Linux的一个非常重要和基本的抽象，用于表示进程的数据结构与内核中几乎每个子系统都有关联
		(2) fork/exec，clone，命名空间，进程间通信
		(3) 调度类，完全公平调度器
		(4) Linux通过增强调度器适应多CPU的系统，以及内核抢占和低延迟特性让Linux有更高的响应速度
第3章 内存管理
	3.1 概述
		(1) 内存管理的实现涵盖了很多领域：
			[1] 内存中的物理内存页的管理
			[2] 分配大块内存的伙伴系统
			[3] 分配较小内存的slab、slub和slob分配器
			[4] 分配非连续内存块的vmalloc机制
			[5] 进程的地址空间
		(2) Linux内核将虚拟地址空间分为两个部分，底部较大的部分用于用户进程，顶部较小的部分专用内核。
		(3) 可用的物理内存将映射到内核的地址空间中，访问内存时，虚拟地址通过MMU映射到物理页帧上。虚拟地址空间的内核部分必然小于CPU理论地址空间的最大长度。如果物理内存比可映射的内存空间多，那么
			内核需要借助高端内存技术管理多余的内存，目前在64位机器上不需要使用高端内存技术
		(4) 从管理内存方式的角度，计算机可分为两类：
			[1] UMA计算机，将可用内存以连续方式组织起来，SMP系统中的每个处理器访问各个内存区都是同样快
			[2] NUMA计算机，每个CPU都有自己的本地内存，可分别支持快速访问。各个处理器之间通过总线连接起来，可以访问其他处理器的内存，但速度较慢，进程迁移要注意此问题
		(5) 如果在UMA计算机中使用不连续的内存，则其类似于NUMA。内核主要考虑NUMA系统，UMA可以被看作特殊的NUMA。目前大部分计算机使用的都是平坦内存模型，为了兼容才使用上面的设计
	3.2 (N)UMA模型中的内存组织
		(1) 不同体系结构的内存管理差别很大，第一是页表中不同数目的间接层，另一个关键是NUMA和UMA系统的划分。内核通过明智的兼容设计隐藏了这些差异，内核对一致和非一致的内存访问系统使用相同的数据
			结构，因此针对各种不同形式的内存布局，各个算法几乎没有什么差别。在UMA系统上，只使用一个NUMA结点来管理整个系统内存。而内存管理的其他部分则相信它们在处理一个伪NUMA系统
		3.2.1 概述
			(1) 内存被分为结点，每个结点又被分为内存域，内存域有以下几种类型：
				[1] ZONE_DMA：标记适合DMA的内存域
				[2] ZONE_DMA32：标记了使用32位地址字可寻址、适合DMA的内存域
				[3] ZONE_NORMAL：标记了可直接映射到内核段的普通内存域
				[4] ZONE_HIGHMEM：标记了超出内核可直接映射的物理内存
				[5] ZONE_MOVABLE：伪内存域，用于防止物理内存碎片
			(2) 各个内存域都关联了一个数组，用来组织属于该内存域的物理内存页。对每个页帧，都分配了一个struct page示例以及所需的管理数据。各个内存结点保存在一个单链表中，供内核遍历
			(3) 每个结点都提供了一个备用列表，该列表包含了其他结点，可用于代替当前结点分配内存
			(4) UMA系统是NUMA系统的特殊情况，只有一个NUMA结点
		3.2.2 数据结构
			1. 结点管理
				(1) 内存结点由数据结构pg_data_t表示
				(2) node_zones是一个数组，包含了结点中各内存域的数据结构
				(3) node_zonelists指定了备用结点及其内存域的列表，以便在当前结点没有可用空间时，在备用结点分配内存
				(4) 结点中不同内存域的数目保存在nr_zones
				(5) node_mem_map是指向page实例数组的指针，它包含了该结点的所有内存域的页
				(6) 在内存管理子系统初始化之前，内核也需要使用内存。bdata就指向在这个时间段使用的自举内存分配器
				(7) node_start_pfn是该结点的第一个页帧的逻辑编号，系统中所有结点的页帧都是依次编号的，每个页帧的号码都是全局唯一的，node_start_pfn在UMA系统中总是0，因为只有
					一个内存结点，node_present_pages表示结点中页帧的数目(不包含洞)，node_spanned_pages表示结点中页帧的数量(包含洞)
				(8) node_id是全局结点ID，系统中的NUMA结点都从0开始编号
				(9) pgdat_next链接下一个内存结点，所有内存结点通过一个单链表连接
				(10) kswapd_wait是交换守护进程的等待队列，将页帧换出结点时会用到。kswapd指向该结点的交换守护进程的task_struct。kswapd_max_order用于辅助页交换子系统的实现，
					用于定义需要释放的区域的长度
				结点状态管理
					如果结点多于一个，那么内核将会维护一个位图，用于提供各个结点的状态信息
			2. 内存域
				(1) 内存域结构中有两个自旋锁，分别对应两个不会互相干扰的子系统。并且该结构进行了高速缓存优化，将同一子系统的成员放在一起，并且用ZONE_PADDING分隔，并且使用编译器字段
					____cacheline_maxaligned_in_smp通知编译器，从而保证同一个子系统的字段都在一个cacheline中，并保证了最有的高速缓存对齐方式。虽然该结构的长度增加了，但内存
					域结构在内核中使用的不多，因此没太大关系
				(2) page_min，page_low，page_high是页换出机制的“水位”
					[1] 空闲页数量大于page_high，则内存域的状态是理想的
					[2] 空闲页数量小于page_low，则内核将页换出到硬盘
					[3] 内存页数量小于page_min，那么页回收的压力就比较大，内存域中急需空闲页
				(3) lowmem_reserve数组为内存域预留了指定的内存，用于无论如何都不能失败的内存申请
				(4) pageset是一个数组，用于实现CPU的冷热帧列表。有些页帧大部分内容都在高速缓存中，即热帧；有些页帧大部分内容未缓存，即冷帧
				(5) free_area是一个数组，用于实现伙伴系统。每个数组项保存固定长度的连续空闲页集合，长度由数组下标表示，连续内存页则有起始位置和长度共同表示
				(6) 该结构的第二部分保存页换出机制的重要成员，频繁使用的页帧是“活动的”，反之则是“不活动的”，这个性质在页换出机制中很重要。活动的页应尽量不动，不活动的页换出则没什么影响
					[1] active_list是活动页的集合，inactive_list则是不活动页的集合
					[2] nr_scan_active和nr_scan_inactive指定在回收内存时需要扫描的活动和不活动页的数目
					[3] pages_scanned指定了上次换出一页以来，有多少页未能成功扫描
					[4] flag指定内存域当前的状态，正常状态下不会设置任何标志。特殊情况下纵沟有三种可以设置的标志
						(1) ZONE_ALL_UNRECLAIMABLE：内核页面回收时，此内存域所有页帧都被钉住的情况
						(2) ZONE_RECLAIM_LOCKED：此标志可以防止多个CPU并发回收内存域
						(3) ZONE_OOM_LOCKED：极限情况下，内核杀死消耗内存最多的进程从而释放内存，此标志可以用来避免多个CPU同时执行该操作
					[5] vm_stat维护了大量有关内存域的统计信息
					[6] prev_priority存储量上一次操作扫描该内存域的优先级，扫描操作由try_to_free_pages进行，直至释放足够的页帧
				(7) 第三部分则是其他关键成员
					[1] wait_table、wait_table_bits和wait_table_hash_nr_entries实现了一个等待队列，可供“等待某一页变为可用”的进程使用。内核中还有很多事件的等待队列，这只是
						其中一个事件
					[2] zone_pgdat指向内存域的父结点
					[3] zone_start_pfn时内存域第一个页的索引
					[4] name保存内存域的惯用名称，spanned_pages指定内存域中页的总数(包含空洞)，present_pages指定实际可用的页数目
			3. 内存域水印的计算
				(1) 在计算内存水印之前，首先要计算为关键性分配保留的内存空间 min_free_kbytes。该值随着可用内存的大小而非线性增长，page_min就是以页为单位的min_free_kbytes
				(2) 数据结构中的水印值填充由init_per_zone_pages_min函数处理，该函数会依次调用setup_per_zone_pages_min和setup_per_zone_lowmem_reserve函数
				(3) setup_per_zone_pages_min函数设置page_min，page_low 和 page_high 成员，对于不考虑高端内存的情况，page_min就是以页为单位的min_free_kbytes，page_low
					就是 (page_min + 1/4 * page_min)，page_high就是 (page_min + 1/2 * page_min)
				(4) setup_per_zone_lowmem_reserve函数为每个内存域计算预留内存最小值lowmem_reserve
			4. 冷热页
				(1) struct per_cpu_pageset pageset[NR_CPUS]
					其中pageset用于实现冷热分配器，内核说页是热的，意味也已经加载到CPU高速缓存，其数据可以更快的访问；冷页则不在高速缓存中。在多核架构中，每个CPU都有自己的一个或
					多个高速缓存，因此各个CPU管理必须是独立的
				(2) per_cpu_pageset结构中是一个类型为per_cpu_pages的数组，数组中有两个项，分别对应热页和冷页
				(3) per_cpu_pages有如下成员：
					count记录了页的数目
					high是一个水印，如果count > high，则列表中的页太多了
					CPU的高速缓存不是用单个页来填充的，而是用多个页组成的块。batch是每次添加页的一个参考值
					list是一个双链表，存储了对应冷热页的链表
			5. 页帧
				(1) 页帧代表系统内存的最小单位，对内存中的每个页都会创建一个struct page实例。该结构要尽可能小，因为系统的内存会被分解成大量的页帧，struct page的些许改动也会导致
					保存所有struct page实例的物理空间暴涨
				(2) 页帧被用于各种不同的用途，每种用途都会使用struct page中的成员，但是这些用途可能是彼此隔离的，即不会使用彼此的成员。比如：一个物理内存页可能会被多个虚拟内存页映射，
					因此需要一个计数器用于映射的数目；但如果该页用于slub分配器，那么该页就是明确只有内核使用，只会映射一次，因此不需要计数器，但需要一个成员表示该页被细分为多少个
					内存对象。因此就可以将这两个成员放到一个联合里，彼此隔离的成员在页帧的任何一种用途中都不会同时使用，把他们放到联合里即不会出现错误还能节省空间
				(3) struct page 结构详解
					[1] 从结构的成员来看，page结构主要有两种用途，一种是用于内核的内存分配器，用于内核的内存申请，只有内核使用；另外一种是用于普通的内存映射，可能会有多个虚拟页
						映射到同一个物理内存页
					[2] slab，freelist和inuse成员用于slub分配器
					[3] flags存储了体系结构无关的标志，用于描述页的属性
					[4] _count是使用计数，如果使用计数为0，则内核知道该实例当前不使用，可以将其删除
					[5] _mapcount表示在页表中有多少项指向该页
					[6] lru是一个链表节点，用于在各种链表中维护该页，尤其是活动和不活动页
					[7] 内核可以将多个页合并为较大的合并页，分组中的一个页称为页首，分组中的所有页的first_page成员都指向页首，使用类似并查集的结构来表示一个集合
					[8] mapping指定了页帧所在的地址空间。实际使用的页帧可以分为两种，文件映射页和匿名页。如果是文件映射页，mapping指向文件的地址空间，index是页帧在映射内部的
						偏移量；如果是匿名页，mapping的最低位会置1 (address_space实例总是对齐到sizeof(long)，所以指针的最低位不可能是1，因此通过这样的方式来标识匿名页)，
						将mapping的最低位置0，得到的指针指向一个anon_vma结构，该结构实现匿名页的映射
					[9] private是一个指向私有数据的指针，大多数情况下它用于将页和数据缓冲区关联起来
					[10] 体系结构无关的页标志
						(1) PG_locked指定了页是否锁定，如果该比特位置位，则内核的其他部分不允许访问该页，防止内存管理出现竞态条件
						(2) 如果涉及该页的I/O操作出现错误，则PG_error置位
						(3) PG_referenced和PG_active控制系统使用该页的活跃程度
						(4) PG_uptodate表示页的数据已经从块设备中读取，没有发生错误
						(5) 如果与硬盘上的数据相比，页中的内容已经改变，则设置PG_dirty标志，即脏页标志，并在合适的时机刷回硬盘
						(6) PG_lru有助于实现页面回收和切换。内核使用两个最近最少使用链表来表示活动页和非活动页，如果页在其中一个链表中，则设置PG_lru标志。如果页在活动链表中，
							则设置PG_active标志
						(7) PG_highmem表示页在高端内存中，无法持久映射到内核内存中
						(8) 如果page的private成员非空，则设置PG_private标志
						(9) 如果页的内容正在向块设备回写过程中，则设置PG_writeback标志，内核的其他部分不允许访问该页
						(10) 如果页是slab分配器的一部分，则设置PG_slab标志
						(11) 如果页处于交换缓存，则设置PG_swapcache标志
						(12) 在可用的内存的数量变少时，内核试图周期性的回收页，即剔除不活动、未用的页。在内核决定回收某个特定的页后，需要设置PG_reclaim标志通知
						(13) 如果页空闲并包含在伙伴系统中，则设置PG_buddy位
						(14) 如果页包含在一个复合页中，则设置PG_compound位
						(15) 更新页标志的标准宏都是原子操作，从而避免竞态条件
						(16) 很多情况下，需要等待页的状态改变，然后才能恢复工作，对此内核提供了两个辅助函数：
							void wait_on_page_locked(struct page *page)
							void wait_on_page_writeback(struct page *page)
	3.3 页表
		(1) 层次化的页表用于支持对大地址空间的快速、高效的管理。内核内存管理总是假定使用4级页表
		3.3.1 数据结构
			(1) 内核假定void*类型与unsigned long类型所需的比特位数相同，在Linux支持的所有体系结构上该假定都是正确的。能内存管理更喜欢使用unsigned long，而不是void指针，因为前者更容易处
				理和操作，在技术上，二者是相同的
			1. 内存地址的分解
				(1) 虚拟内存地址被分为5个部分：
					Offset：页内偏移量
					PTE：页表
					PMD：中间层页表
					PUD：上层页表
					PGD：全局目录
				(2) PAGE_SHIFT指定了页内偏移量所需的比特位的总数，PMD_SHIFT指定了页内偏移量和最后一级页表项所需的比特位的总数，类似的还有PUD_SHIFT和PGD_SHIFT
				(3) 各级页表所能存储的指针数目也由宏定义确定。PTRS_PER_PTE、PTRS_PER_PMD、PTRS_PER_PUD、PTRS_PER_PGD。在x86_64架构中，上面的值都是512，每一级页表都占一个页，
					4*9+12=48，正好对应x86_64架构的48位物理地址空间
				(4) 每级页表项对应的地址空间的大小也由宏定义表示：
					#define PAGE_SIZE	(1ULL << PAGE_SHIFT)
					#define PMD_SIZE	(1ULL << PMD_SHIFT)
					#define PUD_SIZE 	(1ULL << PUD_SHIFT)
					#define PGDIR_SIZE 	(1ULL << PGD_SHIFT)
				(5) 内核也需要对应的掩码从地址中提取各个分量：
					#define PAGE_MASK	(~(PAGE_SIZE-1))
					#define PMD_MASK	(~(PMD_SIZE-1))
					#define PUD_MASK	(~(PUD_SIZE-1))
					#define PGDIR_MASK	(~(PGDIR_SIZE-1))
			2. 页表的格式
				(1) 每级页表的页表项都有其对应的数据结构，本质上可以和unsigned long互相转换，其中：
				 	pgd_t用于全局页目录项
				 	pud_t用于上层页目录项
				 	pmd_t用于中间层目录项
				 	pte_t用于直接页表项
				(2) 内核提供了诸多辅助宏：
					[1] xxx_val：将pte_t等类型的变量转换为unsigned long类型
					[2] __xxx：将unsigned long类型的整数转换为pgd_t等类型的变量
					[3] xxx_index：获取地址在对应页表的偏移量，比如 #define pgd_index(addr) ((addr) >> PGDIR_SHIFT)
					[4] xxx_present：检查页表项对应的下一级页表或页是否在内存中
					[5] xxx_none：检查页表项对应的下一级页表或页是否不在内存中
					[6] xxx_clear：删除页表项，通常是置零
					[7] xxx_bad：检查页表项是否无效
					[8] xxx_page：获取页表项指向的下一级页表或页的page实例
				(3) xxx_offset宏用于获取页表项真实的地址。假设pmd页表的地址是pmd_addr，要获取目标地址addr在pmd页表上页表项地址，计算方法是 pmd_addr + pmd_index(addr)
				(4) 每个体系结构都要定义PAGE_ALIGN宏，它需要一个地址为参数，并将该地址“舍入”到下一页的起始处。假如页大小是4096，那么该宏总是返回其倍数
				(5) 尽管页表项的数据类型可以和unsigned long互相转化，但它们并不相同，页表项的数据类型定义如下：
					typedef struct { unsigned long pte; } pte_t;
					typedef struct { unsigned long pmd; } pmd_t;
					typedef struct { unsigned long pud; } pud_t;
					typedef struct { unsigned long pgd; } pgd_t;
					这样做的好处是确保页表项只能由相关的辅助函数去处理，而决不能直接访问处理。虽然不强制，但这是为了安全性所作出的规范
				(6) 由于内存对齐以及48位物理地址空间等问题，页表项并非所有比特位都存储了下一级页表或页的地址，其他空闲的比特位用于保存额外的信息。体系结构不同，页表项中保存的信息也
					不尽相同
			3. 特定于PTE的信息
				(1) 最后一级页表项在多余的比特位上存储了与页有关的附加信息
				(2) _pAGE_PERSENT指定了虚拟内存是否存在于内存中
				(3) CPU每次访问页时，会自动设置_PAGE_ACCESSED。内核会定期检查该比特位，以确认页使用的活跃程度
				(4) _PAGE_DIRTY表示该页是否是脏的，即页的内容是否已经修改过
				(5) _PAGE_FILE表示该页属于非线性映射
				(6) 如果设置了_PAGE_USER，则允许用户空间代码访问该页。否则只有内核可以访问
				(7) _PAGE_READ、_PAGE_WRITE、_PAGE_EXECUTE指定了普通的用户进程是否可以读取、写入、执行该页的内容。内核页和代码页等都是不允许用户进程写入的，防止用户进程执行页中内容
					可以用来防止执行栈页中的代码，从而抑制缓存区溢出的攻击效果。缓存区溢出攻击，是指输入的内容长度超过了栈中预留空间长度(比如使用gets()输入字符串，字符串分配在栈上)，
					从而覆盖了栈帧上返回地址等内容，函数返回时PC就会跳转到预先设置好的攻击代码上，这就是缓存区溢出的攻击逻辑，我们通过禁止执行数据页和栈页上的内容，从而抑制该种攻击的
					效果
				(8) 内核还定义了一系列函数操作这些标志位：
					[1] pte_present		页在内存中吗
					[2] pte_read		从用户空间可以读取到该页吗
					[3] pte_write		可以写入到该页吗
					[4] pte_exec 		该页中的数据可以作为二进制代码执行吗
					[5] pte_dirty		页是脏的吗
					[6] pte_file		页是非线性映射吗
					[7] pte_young		访问位设置了吗
					[8] pte_rdprotect	清除该页的读权限
					[9] pte_wrprotect	清除该页的写权限
					[10] pte_exprotect	清除该页的执行权限
					[11] pte_mkread		设置读权限
					[12] pte_mkwrite	设置写权限
					[13] pte_mkexec		设置执行权限
					[14] pte_mkdirty	将页标记为脏
					[15] pte_mkclean	清除脏标志
					[16] pte_mkyoung	设置访问位
					[17] pte_mkold		清除访问位
		3.2.2 页表项的创建与操作
			(1) make_pte	创建一个页表项
			(2) pte_page 	获得页表项对应页面的page实例
			(3) pgd_alloc	分配并初始化可以容纳一个完整页表的内存
			(4) pud_alloc
			(5) pmd_alloc
			(6) pte_alloc
			(7) pgd_free	释放页表的内存
			(8) pud_free
			(9) pmd_free
			(10) pte_free
			(11) set_pgd	设置页表中某项的值
			(12) set_pud 
			(13) set_pmd
			(14) set_pte
	3.4 初始化内存管理
		3.4.1 建立数据结构
			1. 先决条件
				(1) 大部分系统都只有一个内存节点，目前只考虑此类系统
				(2) 内核在mm/page_alloc.c中定义了一个pg_data_t实例（称作contig_page_data）管理所有的物理内存。NODE_DATA宏只需要返回contig_page_data的指针即可
					#define NODE_DATA(nid) (&contig_page_data)
					在UMA系统中，numnodes被设置为1
			2. 系统启动
				(1) 在start_kernel函数中，域内村管理初始化相关的函数如下：
					[1] setup_arch：特定于体系结构设置函数，其中一项任务是负责初始化自举分配器
					[2] setup_per_cpu_areas：初始化内存管理模块的pre-cpu变量
					[3] build_all_zonelists：建立结点和内存域的数据结构
					[4] mem_init：另一个特定于体系结构的函数，用于停用bootmem分配器并迁移到实际的内存管理函数
					[5] kmem_cache_init：初始化内核内部用于小块内存区的分配器
					[6] setup_per_cpu_pageset：为pageset的第一个数组元素分配内存
			3. 结点和内存域初始化
				
			
				
				
